{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c7c54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers --quiet\n",
    "import os, sys\n",
    "os.kill(os.getpid(), 9)  # force-restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db963",
   "metadata": {},
   "source": [
    " ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42.2/42.2 kB 1.5 MB/s eta 0:00:00\n",
    "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11.6/11.6 MB 72.8 MB/s eta 0:00:00:00:01:01\n",
    "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 561.5/561.5 kB 30.1 MB/s eta 0:00:00\n",
    "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.3/3.3 MB 60.4 MB/s eta 0:00:00:00:01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361369e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02412e52",
   "metadata": {},
   "source": [
    "/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/val_cleaned.csv\n",
    "/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/train_cleaned.csv\n",
    "/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/test_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54452a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (update folder name if needed)\n",
    "train_df = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/train_cleaned.csv\")\n",
    "val_df   = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/val_cleaned.csv\")\n",
    "test_df  = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/test_cleaned.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f2bcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Train shape: (42380, 3)\n",
    "Validation shape: (9082, 3)\n",
    "Test shape: (9082, 3)\n",
    "                                        Headline  \\\n",
    "0                  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß   \n",
    "1          ‡ßß‡ß´ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ ‡¶™‡¶∞ ‡¶Æ‡ßü‡¶Æ‡¶®‡¶∏‡¶ø‡¶Ç‡¶π‡ßá‡¶∞ ‡¶™‡¶•‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ö‡¶æ‡¶≤‡ßÅ   \n",
    "2        ‡¶Æ‡ßü‡¶Æ‡¶®‡¶∏‡¶ø‡¶Ç‡¶π‡ßá ‡¶Ü.‡¶≤‡ßÄ‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶≤‡¶æ‡ßü ‡¶Ü‡¶∏‡¶æ‡¶Æ‡¶ø ‡¶õ‡¶æ‡¶§‡ßç‡¶∞‡¶≤‡ßÄ‡¶ó   \n",
    "3           ‡¶™‡¶∞‡ßç‡¶£ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑‡ßá ‡¶ö‡¶ü‡ßç‡¶ü‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶õ‡ßá‡¶≤‡ßá‡¶∞‡¶æ   \n",
    "4  ‡¶®‡ßú‡¶æ‡¶á‡¶≤‡ßá ‡¶ï‡¶∞‡ßç‡¶§‡¶¨‡ßç‡¶Ø‡¶∞‡¶§ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡ßü ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂ ‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø‡ßá‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ   \n",
    "\n",
    "                                             Content  Label  \n",
    "0  ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞...      3  \n",
    "1  ‡¶Æ‡ßü‡¶Æ‡¶®‡¶ø‡¶∏‡¶Ç‡¶π ‡¶∏‡ßç‡¶ü‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶™‡¶æ‡¶∞‡¶ø‡¶®‡¶ü‡ßá‡¶®‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶ú‡¶π‡¶ø‡¶∞‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶ú‡¶æ...      2  \n",
    "2  ‡¶Æ‡ßü‡¶Æ‡¶®‡¶∏‡¶ø‡¶Ç‡¶π ‡¶Æ‡¶π‡¶æ‡¶®‡¶ó‡¶∞ ‡¶Ü‡¶ì‡ßü‡¶æ‡¶Æ‡ßÄ ‡¶≤‡ßÄ‡¶ó‡ßá‡¶∞ ‡¶∏‡¶≠‡¶æ‡¶™‡¶§‡¶ø ‡¶è‡¶π‡¶§‡ßá‡¶∂‡¶æ‡¶Æ‡ßÇ‡¶≤ ...      3  \n",
    "3  ‡¶¨‡¶®‡ßç‡¶¶‡¶∞ ‡¶®‡¶ó‡¶∞‡ßÄ ‡¶ö‡¶ü‡ßç‡¶ü‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá ‡¶™‡¶æ‡¶∂‡ßç‡¶ö‡¶æ‡¶§‡ßç‡¶Ø‡ßá‡¶∞ ‡¶õ‡ßã‡¶Å‡¶Ø‡¶º‡¶æ ‡¶≤‡ßá‡¶ó‡ßá‡¶õ...      0  \n",
    "4  ‡¶®‡ßú‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤‡¶ø‡ßü‡¶æ‡ßü ‡¶ï‡¶∞‡ßç‡¶§‡¶¨‡ßç‡¶Ø‡¶∞‡¶§ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡ßü ‡¶π‡ßÉ‡¶¶‡¶∞‡ßã‡¶ó‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞...      3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750e7e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BULLETPROOF Version - Handles all data issues\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set environment variable to avoid tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Load the full datasets\n",
    "print(\"Loading and cleaning datasets...\")\n",
    "train_df = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/train_cleaned.csv\")\n",
    "val_df   = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/val_cleaned.csv\")\n",
    "test_df  = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/test_cleaned.csv\")\n",
    "\n",
    "def bulletproof_clean_dataset(df, name):\n",
    "    \"\"\"Ultra-robust data cleaning\"\"\"\n",
    "    print(f\"\\nüßπ Cleaning {name} dataset...\")\n",
    "    original_size = len(df)\n",
    "    \n",
    "    # Show initial state\n",
    "    print(f\"  Original size: {original_size}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Handle missing values in text columns\n",
    "    for col in ['Headline', 'Content']:\n",
    "        if col in df.columns:\n",
    "            missing_count = df[col].isna().sum()\n",
    "            print(f\"  Missing values in {col}: {missing_count}\")\n",
    "            \n",
    "            # Fill NaN with empty string and convert to string\n",
    "            df[col] = df[col].fillna('')\n",
    "            df[col] = df[col].astype(str)\n",
    "            \n",
    "            # Clean weird values that might not be strings\n",
    "            df[col] = df[col].apply(lambda x: str(x) if x is not None else '')\n",
    "            df[col] = df[col].apply(lambda x: '' if x.lower() in ['nan', 'none', 'null'] else x)\n",
    "    \n",
    "    # Handle labels\n",
    "    if 'Label' in df.columns:\n",
    "        print(f\"  Missing labels: {df['Label'].isna().sum()}\")\n",
    "        df = df[df['Label'].notna()]  # Remove rows with missing labels\n",
    "        df['Label'] = df['Label'].astype(int)  # Ensure labels are integers\n",
    "    \n",
    "    # Create combined text and check for validity\n",
    "    df['combined_text'] = df['Headline'].astype(str) + ' ' + df['Content'].astype(str)\n",
    "    df['combined_text'] = df['combined_text'].str.strip()\n",
    "    \n",
    "    # Remove rows with empty text\n",
    "    empty_text = (df['combined_text'] == '') | (df['combined_text'].str.len() < 3)\n",
    "    df = df[~empty_text]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['combined_text', 'Label'])\n",
    "    \n",
    "    # Final validation - ensure everything is clean\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    final_size = len(df)\n",
    "    print(f\"  Final size: {final_size} ({original_size - final_size} removed)\")\n",
    "    \n",
    "    # Validate the cleaning worked\n",
    "    sample_text = df['combined_text'].iloc[0]\n",
    "    print(f\"  Sample text type: {type(sample_text)}\")\n",
    "    print(f\"  Sample text preview: '{sample_text[:100]}...'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean all datasets\n",
    "train_df = bulletproof_clean_dataset(train_df, \"Train\")\n",
    "val_df = bulletproof_clean_dataset(val_df, \"Validation\") \n",
    "test_df = bulletproof_clean_dataset(test_df, \"Test\")\n",
    "\n",
    "# Show final stats\n",
    "print(f\"\\nüìä Final dataset sizes:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Analyze class distribution\n",
    "print(f\"\\nüìà Label distribution in training set:\")\n",
    "label_counts = train_df['Label'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"  Class {label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "print(f\"\\nü§ñ Loading model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class BulletproofNewsDataset(Dataset):\n",
    "    \"\"\"Ultra-robust dataset class with extensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        print(f\"üîß Creating dataset from {len(df)} samples...\")\n",
    "        \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Process each sample with validation\n",
    "        skipped = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Get text - use the pre-cleaned combined_text\n",
    "                text = row['combined_text']\n",
    "                label = int(row['Label'])\n",
    "                \n",
    "                # Final validation\n",
    "                if not isinstance(text, str):\n",
    "                    text = str(text)\n",
    "                \n",
    "                if len(text.strip()) < 3:  # Skip very short texts\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Test tokenization to catch issues early\n",
    "                test_encoding = self.tokenizer(\n",
    "                    text[:100],  # Just test first 100 chars\n",
    "                    truncation=True,\n",
    "                    max_length=50,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # If we get here, the text is good\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping sample {idx}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Dataset ready: {len(self.texts)} valid samples ({skipped} skipped)\")\n",
    "        \n",
    "        # Validate we have data\n",
    "        if len(self.texts) == 0:\n",
    "            raise ValueError(\"No valid samples found!\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"üìù Sample text: '{self.texts[0][:100]}...'\")\n",
    "        print(f\"üè∑Ô∏è  Sample label: {self.labels[0]}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # Final safety checks\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            \n",
    "            if not isinstance(label, int):\n",
    "                label = int(label)\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"üí• Error in __getitem__ at index {idx}:\")\n",
    "            print(f\"   Text type: {type(self.texts[idx])}\")\n",
    "            print(f\"   Text value: {repr(self.texts[idx][:200])}\")\n",
    "            print(f\"   Label: {self.labels[idx]}\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            raise e\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nüóÇÔ∏è Creating datasets...\")\n",
    "train_dataset = BulletproofNewsDataset(train_df, tokenizer)\n",
    "val_dataset = BulletproofNewsDataset(val_df, tokenizer)\n",
    "test_dataset = BulletproofNewsDataset(test_df, tokenizer)\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4\n",
    ").to(device)\n",
    "\n",
    "# Smart epoch calculation based on class imbalance\n",
    "max_class_ratio = label_counts.max() / label_counts.min()\n",
    "if max_class_ratio > 10:\n",
    "    num_epochs = 5\n",
    "    print(f\"\\nüìö High class imbalance detected ({max_class_ratio:.1f}:1)\")\n",
    "    print(f\"   Recommending {num_epochs} epochs\")\n",
    "elif max_class_ratio > 3:\n",
    "    num_epochs = 4\n",
    "    print(f\"\\nüìö Moderate class imbalance detected ({max_class_ratio:.1f}:1)\")\n",
    "    print(f\"   Recommending {num_epochs} epochs\")\n",
    "else:\n",
    "    num_epochs = 3\n",
    "    print(f\"\\nüìö Balanced classes detected ({max_class_ratio:.1f}:1)\")\n",
    "    print(f\"   Recommending {num_epochs} epochs\")\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 16\n",
    "steps_per_epoch = len(train_dataset) // batch_size\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"  Total epochs: {num_epochs}\")\n",
    "print(f\"  Total steps: {total_steps:,}\")\n",
    "print(f\"  Estimated time: ~{num_epochs * 25}-{num_epochs * 45} minutes\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bangla-fake-news-final\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=num_epochs,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,  # More frequent logging\n",
    "    \n",
    "    # Saving and evaluation\n",
    "    save_steps=steps_per_epoch,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=int(0.1 * total_steps),\n",
    "    \n",
    "    # Early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # System optimization\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=[],\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1_weighted,\n",
    "        \"f1_macro\": f1_macro\n",
    "    }\n",
    "\n",
    "# Create trainer with early stopping\n",
    "print(\"\\nüöÄ Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Final validation before training\n",
    "print(\"\\nüîç Final validation:\")\n",
    "print(f\"  Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"  Val dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Test one batch to ensure everything works\n",
    "print(\"\\nüß™ Testing one batch...\")\n",
    "try:\n",
    "    sample_batch = [train_dataset[i] for i in range(min(4, len(train_dataset)))]\n",
    "    print(\"‚úÖ Batch test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Batch test failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "# START TRAINING\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ STARTING BULLETPROOF TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Dataset: BangFakeNews ({len(train_dataset):,} samples)\")\n",
    "print(f\"ü§ñ Model: {model_name}\")\n",
    "print(f\"üìö Epochs: {num_epochs} (with early stopping)\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: ~{num_epochs * 30} minutes\")\n",
    "print(f\"üìà Progress logged every 100 steps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìà Final training loss: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nüìä Final validation evaluation...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    for key, value in eval_result.items():\n",
    "        if key.startswith('eval_'):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    print(\"\\nüéØ Test set evaluation...\")\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    test_preds = test_predictions.predictions.argmax(-1)\n",
    "    test_labels = test_predictions.label_ids\n",
    "    \n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average=\"weighted\")\n",
    "    \n",
    "    print(f\"üèÜ FINAL RESULTS:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nüíæ Saving final model...\")\n",
    "    trainer.save_model(\"./final-bangla-fake-news-model\")\n",
    "    tokenizer.save_pretrained(\"./final-bangla-fake-news-model\")\n",
    "    \n",
    "    print(f\"\\nüéä SUCCESS! Model saved to './final-bangla-fake-news-model'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nüí• Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Save whatever we have\n",
    "    try:\n",
    "        trainer.save_model(\"./emergency-save-model\")\n",
    "        print(\"üö® Emergency model save completed\")\n",
    "    except:\n",
    "        print(\"üö® Could not save model\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üèÅ TRAINING SESSION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAADhCAYAAADS+V/xAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAEOaSURBVHhe7Z1/aB1Jtt/PW15CXuL7MpFuYHdMxtaCYr0YLCysMCyekR1BYIzXCIvFEKPxHzEyfvMgCAfPXxFC+WtMzCWQfUbC+cNz8R9mkLl4Bw8EFFszZhliIyODEzmClTzBMwu5cibvOj8Ij/dSp350V1X/vvdKVkvfz9DMbXV3dfWpU6dO1Tnd/oOenp6/JgAAAACAEvIT/X8AAAAAgNIBRwYAAAAApQWhJQAA6CIT80s0dawifrXo8fURunRb/Z1omhaWx6hP79FGg4bOzqrfMwu0fCY4Imk9qdHIZF388q6zscs4P0dLV4aJ78ys3xui8Rm94yHrWF0Mr02rW9z9W4+pNnKJ6jRBc0tTNGxuahOcw3jnOeWnE61rDKnyCwnbhvHbJ8Q9L12WgXwKPNP03WUaO6h3DI68thrVHgMvojJKJaecFSyXUWpKGafokK23BWRoA0cGAAC6SPzAqww5JQ2IPEAcWSlgxO1Bwuxb5cvBYYBWYwfqjGsj8PFBWhkap3y1U4Nk9UFYnhy4yQxSOQfRIgNcDvnJdjm0msNZEPW7e5wendXnpcpSl8tOT0FHZrRZ0InoKh04Mjn11O0H2ToU32/ykcuR2f/zf0RH3v8nVPnjv6f/AgAAe4vWX/wPevbtf6RXv/vP+i/xxBnkrIGrqBH3z0+6Z+g8WPBgdLIZDOiZg6ocyKu0mNeR8cqPvZ7POUPUSCwzdK4a72bLJlN+RZ/BIWXQ1+U2N/qoL07WCZTVkcmvp6r8wJnNIf+ifcAm05H55//yGvX84V/Q3/z7h+gnfytcagMAgL3EX/3fFv2///6CXv/lH9O//zdX9V+jRA1yt2ejXJ69opIwMPoOhcY9N8dqSyEnwBvAmNh6RJ8hiTjZSCetNwzFZMmPj1+km4mDtl+eS8wzScK/rxxJcBoTSHdkjBNXo+bJMBwXF8KR9bZCVHEhMCmbSJgsdGQWq1NBGclhIkVuPfV15m06Mn/2r/4t/d13eujvHPqn9Ac/+UP9VwAA2Jv89V/9Jf2vF/+B/uePr+nf/et/of/qEjHIbMQniW4+qNLUGZMlsO6sRviDTWqYIuIYJAy0cQ5E3ACTUTd1TZh7EzluEzNgxQ9QnTky8m9WmChLfjzgDz5Ldgz88hwSHEK7XtKh6LIj0+fk8Ki/VYM6qzYf3rTuqdupaTkzSi7NsL3EOXP9l+iSdmRYFoHzw895ppqYN8RkydkQeb4cOmTLsyiJby199M/+jCp/9Ddo35+cghMDAAACtoVsE9k2so3MRX+VKpVhmuLcgqEhudWeVGlsaU4MR4r65EhwbGioQesHx2j57rQ+aiMGoJN9tP4gZsCNo1J1EiwnRgaosrESDiA56ka3L9FIULchamz00djyghhao0yPDxM9acQ7OREqVO3XPwsi5WU5Funym6D9vUR9Zy4SzYfnNI9N0YIe8P3y5MC7vEzLvLFsfAdHHL/IDkIbg66hIu4vy9fb0nwgcUnryU3LoZilxpMWVQ4dV+0yMyacEOEM2PcX7XRTnNN3xDz3NI0dI+GYWA6DOOeS7ewKRyRwfmca9LhVoYERtx42+fR0mgYPtmh1yZJYAR1qh0RH5k/+YT/90Xv/WO8BAAAwsG1kG5kfd9CpTy7SunAgxuxBJWCWxu+tEx0cjBr688dpQAxgK7HXxdBqijsbJuj4oQqtP/MH3yJ1E7U7KwYw4R4NRo7HDGCptKi5pn92lXj5xTkGfScth83GHnifDUpHwzg9/JwLMvk3YVUqJ7wiFDoFQ5HVmeb37n79+6b+JVry3arXtgp5Tu9+7ewMilZq0quUFa9W0y+hCAl6yvdtrdKjlPsm61B7JDoyP/nfv6c/fOcf6D0AAAAGto1sI3MTGXTWqdnSP+NYa4phPkpkRUVSp1ebRNV3k2fSEnaC6DE1/MGjaN2SjicMYM7gul3EyC/NMUhlZpyGxIBtnJ6J+VExBFdo+Eq4miJzTHh1oourDJlsvspelYtxdrpKjJynj+RZMczSsWLgg3gAALCVzKzQemWAjp/X+5I+qlZSViQ45KN/hiStqIhhQYwKlar7fQ8eUFovHgUDigz7WPuSduomj+ufFv79Aniw80JceWbtHeHIr06PXtghF4Vc1cjjDHi44RW1NTbEAc4XScod6gJ2faUTFrNi5zyTlLvftl3G19PzczR6MM+KYbwOtUuiI/NXf/un9Jc//je9BwAAwMC2kW1kPmZpZUPM4CfDMMb0Xf6A2qIOdUzQ3F07xDFNC2eEU+DnmsiwUryDUV9apdbBsTD8oQeUxSBckRT2yaqbqN38As1Zg6E83vJXdrj86KqH5PYjWm310ZiVs1IozycGTgxdDvJ4suUXJ5+LnD+yoM5wyhPHFpx8FVVekfrK8mJznPLTd8Za3fHqq/JZbJkK/HNuX6JFr235nLlMJyOJbDnHrxiyPPLoUPskOjL/5b+u0f/57j/pPQAAAAa2jWwj8zJ7VszaN4dpyoQi+FVfO1GzNzy2rL+fEnmjhWe/SasYnNNx/TFVz+gy/BwOXgGxnBObzLpR1Q2j8Ns5keTX/eKspJl4nS6N1OhxL4deuAz12q//mnBHZMkvIh9+uyfh7Zzbr4icRNy0jwXG01etZOaf+Mm+flhq/d4KDZpjkfr6MuVzoh/t89uWz6FO8pJS5cwrhpYj5ZBDhzoAr18DAEBO2nr9+q0jZtJxr2eDLYLlzW9IJb/GnM60/o5Mydor4TX1vHTSb/BBPAAAyEGhD+LJb20k/1s+28r56LddwFYiHJGl/dRoe8WhnI5M+rdxUpD6uQ3/1hL+iQIAwF4n7z9RAEBnlHRF5i2Sy5EBAAAAANiJ4PVrAAAAAJQWODIAAAAAKC1wZAAAAABQWuDIAAAAAKC0wJEBAAAAQGmBIwMAAACA0gJHBgAAAAClBY4MAAAAAEoLHBkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKUFjgwAAAAASssf9PT0/LX+XYAJmluaouGK3jW0HlNt5BLV9e7WMU0Ly2PURy16fH2ELt3Wf+6YhOcKWKfG0DjN6r28TMwv0dQxUehGg4bO5rt6+u4yjR0UIn1So5HJrZLoVsmx25h6xshjZoGWz/CRrLbxnzXHs5uyO9TrbWnLLtV1ewnbtd2+BXYAQR90Wb83ROMzekdg+kHXdLSr93Vtf7Sv2roavQd4u2BFBpSAWVrZUL8qVddwTR/R+xsrO2AQZGO4TMvLSzR3Xv8JJDMzGAwMJH4NYmDYlfBEbnlZOxPbSCf3rRw6LnqzhaOrYKfRsSPDnunQkN5KMxNMok6XRszzNMQckeEZu/lbezPG+uSIuj7nagwze1bdc+tWY8rF7DPVGnRwUMyNDBO0v1f9Wn9WtGVmaVy2aTdXovqoGrOah7aMQzh9J3loaNH6Rkv+pe9I2LKghPCKh+xTanNXLMJ27jpdvW+LWnx6ZYCOW5MRNWHSx8COY0tWZIwnvHx3OvwttqV5x8dVS4P6mNoWrEFK458jynThJT99bGnO9aK3AF6m5Hstzc+p+8p7mpm4tVl1seUhOT9HS/I88bzBb/ea8D7qL/a++c3bgtNpLVlw2UZ2HcjFbr+4suy68Ba0sd9uHdRBMrOiHUtr5n7+OA1Ix2GdVsTfInVNXRkxsrLP8eSn/xqS0s6yHc3Sc4WGr4hjur39tmTS5WrXLatOebHLUZujO7Yeyi28V2Ibd0LQdk1aWVgVQ4TAcVI1fr1sOcUeC9soeL7gPPNMlnznlZ7KZ4rYI79/CWLsUdCWdt260Pd2C2oiN0KNpv7DNtHufZub0pOhgRHTctM0yKs6G6u0qv5gEe1XkTEqQYdDu2CPJUzUznSlz+1iOnZk+s6kCPvgmMoN0VSOTQWGQXb+SHyzj8YsAxp/jg0PGGHckirDdHGbGrxybDi8r/gVmYmLukxFnC4f8bxXhsVTaMQ1Y77h9GAZ2kulfSeN8nOHsmTBZafKLhvuaHb7Sfi5dIfj9oldtuWO2+G9o4ThJTNznxgZULLTYaW+aCMI/cg7+OeRX7vt7JIl1xBPv8Wv0Xb0WxpSuxwF913VH8Wz23pokdjGHeK03e1HtCo9GS+8xM5AQr1Sj+VGyPdYKJWJd6v6V0jfmdDRTbJH9clF5WRbs3gT8lx/UPZV6gKwDgeDbydOd0G6fN/mM+VYB+ElHVZaf/aI91zO76eI1ohxLxgLc+ipO5awHYrmabLdhzOTzNbmyFhLfg1nEJqg44dUS3FSlVoSrNFjbcyUsZ6mMW3sw3PE5oVnTGjLlO/nUGwZ5tlkOM2EKfR2T4dBevd7A5OPCVuZZyeqvpuhrJwwbN+jUlWdIIjhctKkqoeRSXvoWYggDB/qcJswHK7DFd7TD58E13Yh7BiEl6RcQx0yYSUTwnHqKszM/sRVGYsY+dWe6EYJSGnn25doxA9HxoYSi8g1PKcT/Q6cBqs/mmcLHWFF2Nf8MGpyGxfHb7s6PXqh6xOEl0zoSWB0njepR2nHimGel58pCAHLzfTJClX7+f9p9sg42WYWb9pYrRSCkrGmHWvtmCqnNKEtZb83OhP2K9VPc+qpPZbE2CFjZyrHxoRmgTi6miMTMXCbr4IGW2+qBlaYmW2LVpfMGXV6tal/MoGnu06LiYazRc019cstf+tpvXhkKaO3vJh7NaJJr2R+RvjsWQNVq6kHz7WmnDWYgTqYTVpJr8HA3w6W/MMOvE62mIOZqGjPMf3scoYvOveicVzNil3BVYtYTHiJDcyMG1Zi3BBIdAUijTj51b/316TbbWeLHHINidHvTOc4ilmpsnXWfbZZahgDLGZ+8tn06lBiG3dCEFYK9SNYoQrCS6GNeLzgulTpx4pg2x+hA7ziop9x2Z8VZ9ij2YXHsj/KWbwZjHZEAvo24uSqbOMbaF2/r3Gs2TGd02GlhLb0wkbuSms+PbX7ZZwdCm09SGJrV2QCo2utwMiB2BhuOw4ZJm5Kbr8SwzxjLzcLD3d+5/mkE/Oj0nAZpy46k99GrDyD4I2edoiVvx9aCVcoghW3M2ppN1gdMasWB8c6HwDF/YKZ70k3rMQGZVQaHD3zua4GlsKkyK8r7ZxLrt3FOEH2mxh+GCVYjTByq5gwbXIbt0uwQhSLkUucjRBtMs8OVtqxELO6mX4/g15xCQZFs7qmybJHxnkXTvZFncTcmZMF3ib1JR1e0mGfpJcJpsc5bKRXYK0+osinpzbBBMPOF+uv5tDfvU1Xc2Qi8UlhDFXs0sxuzAwoXEoOZoDBOWbGY+VEBPcQ57j2d0cRmV1uI6bjsaE1M+f8uQ06OVVfp2bjcfLXqxzC2DeEMbdnsMG9Wk1at2cpwapFuLrQCWaVqVJRMo4YGOEwyfsWzJ8oIr/sdtbyjF2FypZrRwR9Tm0cVw+ezTpm6q5yOKyVJktuze/ryW2sfxYnLqzsOUoyvBRnI0QdDvFf0o8Fq5v62JT4uzo7B4GM/BW9bHukdLMidFP8r7VKj3bsd5lAJkHeFpMVIgztp2sz0vQ0AeulBmOHjA1tPWkILQRxbO2KjJgd2x7q+r2R4FVXOQM0s3WDnA2FS4M8q/dnvUFoZQcRLr8L+Bn859oOOFZr35cHxQ5XhuLkL1c8EnMR1qmRcMxu+44IOjpjGRgrnCUeXsyQvBl1FjHy89sxu53DEE0axeXaIfxs/gqDlFHyR73YyYg/ltzGuZgZ8yY1IeEr9qMywTbWRuhwddqx2bP2s4r6zkffNYnitt36vTBvzZBpj2YawTV7Ksl3VxI6IWkhQhNSlIg+7OtHmp7GwyugUd3jVWB8viGZNr/smw7P4uSMj41zgW+ngG7Cs2w9q0Q7ALD18BsqcvbMiZrbmCMCwB5na1dkwDYy4X17wCyNI1YPwJZivhmDEAAAbwU4MrsZGarrUkgHAJAJh+QQAgBge9mS0BIAAAAAwHaAFRkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKUl8Tsy586d078AAAAAsNO5c+eO/rW3wAfxAAAAAFBaEFoCAAAAQGmBIwMAAACA0gJHBgAAAAClBY4MAAAAAEoLHBkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKVll/xbS9O0sDxIK0PjNKv/UlpmFmj5TJ/eWadGnmeS11Tp8fURunRb/81i+u4yjfU+ptrIJarrv0nOz9HSlWGqyJ1W7PV87eCzIRqf0X+QTNDc0hQNqwup9aRGI5N2ydweYxQ8xT3/+r3BxPwSTR0zQoqRfwxxbeWUE9NO8pqDemejQUNnLY1x9ImJb+egTTfTrs+pj4XJry+uLPz6uOUwrm66x329dctm7PK9smPas532zkP+cos8X4Ye+ffx9Mhuo6jcNHYZ26JHhvz6FKBtYdM7N032/nO793FtpMTvm5p4G5velsCl9CsyrEzLtoEpM9yZpEMyRENDQ1R7UqWxpTnRJdIQHeZkytOLMkeNcbJhw3JlgFb1vYaGvMGN67JsGTaL6bt6wJPXNah5bIoWnA48RlXR8eTx64+pemaJ5s7rw3sFId+pY01hsJV8G5vDNHV3Wh9MILatpmmsuqhlLbZ7TRq+siD+qo/y4EOmLcR9xN7SfKgxE+9WlQE11/vtbJgZc40u05Y+FqWAvngyjdTn/H6qykHSPOuQZfy9+0T0lqivWpEDhrl2yB5sZwaJxECl/l6jxzRMFy05t9XeechdbtbzpesR21HpQMfeR5R9pBkc89uoPjkSlis3IZ+WGNgf6EF/W/TI0J79mR43EzqLVNlP0PHqaiiTe+vUdyaUp9AmqlbYWdTHefOdmEQbm62rwKXUjoz0iA8JZRLKKvpN6ZkYGaDKxmIw0NQnF2m9MkDH0zohD0Cb68J8x8MdtLnhHxUdRTg/6/cSBjXuYNLJUQbJZZoGD4oOumA65Sw1nrSo74juwueP00BlnRbNAHL7Ei1uVGhgZGvM1k5l+kifGBQbwUA4uyB09OCgZeiixLfVLI07KyQN0SZ9NCiNmt8W6j50bMy5T6uZpB0G1gfhAmy4jd2WPhalgL4op2wlkGl9aZValao7iWk1E/qCGlhWl4xjM0srG0TVd937NL83xz1mxq0Zc50evWhRpRreuZ32zkP+crOeL02PtBP34lGw2jD7TEixd7/QDKZOl85aqzNZfZptknD1GrrsbdEjQzv2hycQvcKGerYuXfaeTGZWhN5Vab/zTE16FWdfmVQbm09XQUipHRk5E+jS8u3bR3j4hyq0/swyNlKB0zrhNC3wTGdhRe97iBkFz7Iaz/S+gTu7ZWgiiM4/kjhzH6S+1io9so7JAUV3cGW0wsGGYaNYOXRcG8W9gHIwQkMkuP2IVq2BI0JSW0VQRq65pnd9br8S5jM0qDxAZTExf1E4w4vUaOo/SNrRx+IU0RelZ6PB7DpybX81OqsOUHUfHjcugN9GE7S/V//MZJrGjpHlQLbR3rkoUm7W8/m4eiRlbjnAchC3HBsXJat4p09PksxqzDbpkaG4/RH1nRQTiAcN0W9sirXpxPwo9VnOmlodTCHNxhZuS4Bk3xIzfZeXH28mdAbt5MzHOHps8DdfUd/dZVpe1ls3lsFBm6S0lYc0mIEj6Rs8fVz/NlSOTQXtHFmeFjPDi7x8HhO733Gw8b++SgNX1LNMcajEr3dlmKYSdHr2rAq9KVmMUjMmV6jvjL52ORqO4DCeOsb5eEmD0Nsjz/MZXD0SzIzT0D2iMXntMo02U3IyZBhynVbinDRvNWano5z4RnYOTRw6NMTyukg3Y/Jf+gJ5LhcMpRVpSwBHprzoGfzNBGOT7uQIDo7JBDMVg63R4143twJsH5ltpVHJhcLpsFYhZ8+qtlMGjw3qqhNaYYMYxOhlvoDtzOjZ6L2tTLzsIkLnlyeJbprneTYontnKS+DB2BzTOh06M+JZl1RSpTp+k2hy2dL5Ol0aMcfEJnNIXGcmlOUKDQpZ76z+kvV8IXF6JHMNj6zoa4fEsHwxfvDlNpD5LvE6wys54WrMDqdTJ16uqljysnXROsabzK/J7czkb0uggCNTAiJLuDwTSJnBm8S9JCdH0rJnTcKIP+gw/JOYm6DZfFUO47alRENCudpKGza5AhF528MdgEeWiKpJsXlhXG9a+UwmabvobDQxj6SbRPRFyMAJWQiE49LYSArfCLnMWzkNkZUCdZySdF6WnRT+mKVx4RT6uUhRUkKAHRFTbq7nS9IjP1Qmrp68KUpz81jkitRJTvpNWh3gEEjCSk0M26JHhog+TdOCfEupqBMf36ac5sC6OJrgbMyebeTPCyqqqwCOzM6hTq82/YQuFRuNDH4cBxb/DesldvXWltpfmv9zGZN2ltj5tUe5r2eYa832k6P5Wi/BUtZHG4r6900rSVAh4+2ZCae7iXVqtipU7de7jExC9B0MlT+Q2lZy8Jmi6gPhqOSYOcblCPiotmDdEv87GK7myFdJ5T7PLPPrYydsub5kOdgZ5Bts87Z3UbpZbjE98mEnhsNNqTmJnD8X0b3t0SNDbn3iuor/haFE9aq03JereFvRpp3qA0gCjswOQr1xcjFMZrTj2DIeqwa36OuOwtsXrgm/6jcy+afuEjlv90Qn5m8gmJkUJ60Jnz98fVTNdpOT+yzErH5RzDzGgiV7ns1ZyXz8NoRdtqj36J5LVBMzqAfu65jy9U5j5Hl5XjsLmW1VJOdALpXbM2vRrvPWmoE+rtpilsbt+4qt9kS4t/JVbTVLTdXHbpGhL3IVQOqaelOo76S1PM/hVWsFYGJ+LpC3fPbJYSKj0/xWiXAQxwI5esfFlXP2bNore/quFTbQ14YOY0Z7t01ePeLfGc+XqkdxuVYXxfmh7RkVsgjeBEogyQHdFj0ypOqTkMmSDtE4YUje1NtD/C0Y5ehlyF6UuxDRF8vOzcwFz8tw+Djq5CWQqavAB47MToLjqjI2r2fI/Gr5lryVxQNojVYPmSTQKRp4kf+DS25expj8vob9auqlEfXdA3lcvmK4BxPVhKGU38uQMtLfemk3Fi9XaFQ5waYdSZnbYP4WJ+tD4YpL4bbYFn3Mry/swNdeDFirV36uxkCYXBnRaXbcGkT2DHyz4eh81UqKXj5DZH+0bb0ZtqW51mnPbra3Te5ys58vTY/c5FJenXNzaJzEVbM5OR/8JlPChGXb7BrTRfuTJvvbr4RzZusL66J9n6q1Yl5UH3K0JXDYJV/2BQAAAMBeBCsyAAAAACgtcGQAAAAAUFrgyAAAAACgtMCRAQAAAEBpgSMDAAAAgNICRwYAAAAApQWODAAAAABKCxwZAAAAAJQWODIAAAAAKC1wZAAAAABQWhL/iYJ33nlH/wIAAADATufHH3/Uv/YWWJEBAAAAQGmBIwMAAACA0gJHBgAAAAClBY4MAAAAAEoLHBkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKWl3P/W0qe36OFHB/QO0cuvTtCFz/ROWXGe6SXdP3GBruk9h1/V6MtPjtI+vUvf3acTH9tnXqVbD09RUJIvG+f6N/T016dp6gu5I7n6+UM69Z7eiZR9jmpfXqaj+uZvnt6g01N31I4k4957hHO1L+lyKCS6cXqKbCnFIeXe457rtEVMWwVI3elNOM5tcpiee/rk1DGmbPd4ij52RH59Sa+PWw5j66Z7bVafiB5Pb8/0e3dC+n1t3Dr490/XI6/+Bfu8L1u3bdxrJZHyu0l+fQrQbb8Zc26sLfTGnhDz3J48U9qNyz/8PK6OxZ8D/9ZS6RCd4/BrunHiBJ3g7ddPqfejL6n2K324jHBnkgOReqYbT3vp1Jc18aRRzv2ih9b0eSdO3KeX752iW5/qg7oDkFB8c5xs2chO2x9e/+s16v/klrhKITsuiQ4rrz0hfp2iL2thLa5+LozSa3P8Pm0evWzdm43WKeoVhk6VvQvapR2Eobt8dFMYNS3D10fp8udGwgmIdvkwGGgMV+mw1RY3nhIdvRCnE0LuH8QZVnFEDDIPbaMacJVO9X4dlH3iq006aumB/wxp+tg+BfQlqz6/+hn1yoFEP4/YwsH2HL3fuxbai69e0oGPrGcVv27pgSyuHnKg7g+vv7HW77Zn6r07ILceeXKM9Mt0PTpXO0yvA3tyg572FOnzRAd690nnxpR/wnEwD1DPPnaczDGxbZkT0579ufpLa1IYwGW5tjCo92cXwr/p7f537K/cV8/96WHL/gp50lGasOQpYTv80HYuLeQx24bncMb2MCV2ZO7Q1MeWh/vFFH393T7q/0V3zex2cu4X/bTvu6+DWdKdqa/p5b5+ej+mE96ZumDNpq7Rc9GJen+qn110ogNiBnA/UPxrdJ8N1y+VAfTvo2R3gD6UHU0YvPeE0flNaGiu/YYvPqWNvn+cy35DBw5r4/qr96l/30v62hjxXdAu7XD18IHQqAlYhm/eO6xlGA8b083vXuo9wzW6YBn9O79dozf7eqJOyaenxEDzUgylLsEALAz6G/23ELds+uw+PX1zgA7rAercT3vFDPR58AyJ9+6EAvqSqz5vXkdkoPDsxWfPxXm99DPTt7Qj8tz0mS++pbU3+6jn57wjnKD+ffTym/B62Tff+9AdIBPv3T759Ug5C2u/NTX0bILX1r7sXHsiZPXNS9rX/752dDL6vGbz94F0Y9ikH4Lyt5B27A9PIHpE3/E7CPcp4YLcyON0yUmIdV/h6ISOxx36du0N7eu1NJUdFTmZFE5OtGNKW0DCGYPzko9dlCNzjn7Wk9WZdjLaWD63Ow0boxxOgN+JYrjz+02inp9pwxTl5abX0Wy++EGYIW30pZO0Rt9aRkkaRW1clZMUDjbMtee2UdwLKMMfDioCOTCGTkIEMfPmkNL953o/AWXgwoFNcZVu8Ureb6IX35k6TSdyhLQUajB8/Tu1p9o1HKzj2rZTiuhLZn1+3hMzq47nXO1DOuA486p9lDMv0ANi4NhEeEmvA0dHUODe+SmiR8pWmMlK7LUW8XoUIp3G1z8ovcno8+JsaXsTkU7i9lDc/pyj2gUxgfjmPr3WfzGwE2k7r2mky/MqneLD1uSQHazTJxJCxLLt0u05cNk9jgx7z6mGZ7ehlj0fPhTbBaK6vZTLs819R+lUYOzEuVbYwR8QVEczZtg3iOJqNvr6N9gKtCNyK8FoCidHtrPYOJbuhyyufs5L6fUEo5gf2c72gMXGVoYd1b0vcxgqz+x0q8hTH6H3l7WsHvohGJ4F62MTVPeuvUNTp2/QWv9lde0nPfR10KfUjPrAB1YYS9ob/duQdu9t4NrHKgws7//wQ3rt50pl6FGIsgfupCqbAx/pZ38YF8o5QKf0vR92PTzZPudqEzJkFl35YOdMOPUU6kz8cwkSJpIcolfXcW5aktMSAzt+b4RbJUPCuowdJLOdyO5wZLiDytwSOy6722HDq+Ont4gmhLKHMetrdEHGhk1HmiBasxa9eUCQ+RDm+GF6ba1vXvtYxchNJ5qgta4vmYOQTEfEisc/PyzaxDZqeiWn3uHsTSVrbtJ9e/WG+5V0krWePT8s9MHOK9lmsurj5C1oHbYdCjkLVsfrQqvdZ+G8MtFPbpnrn9Nhq0/xypbMT9F94uHh125IIOveW46a2LCDoupQJ7rw0MlzSdWjAJVfxzkm+cMali3iTdoWa9C35M6blONOGJiFAzLBOp/onIsJ3QeWvvk5ZBpeBYpbjWHHUj2z0iWnLbIQTvGH7GzbMnsLznFZKL0jI73eDzjpt4DHWzIyw2XSMXnpzhgd43GavuXFXbNUzDiG9wL9wMl6m8ZdcQ3T6d+SuDojxp2VH2Dfe88Shm0M7EAUcUSkk0l6tY1XGNJWcnKhBkC5uuEkaKpVPGdpXejM/e9SwmPdJKIvResjdPhWcl6SdEyCvDBuBy/UxJMB7lNWHkg4MInt4x+cMJxL+r07J+a+Op8jzItTdaCEkIqjRwa5YsMrOTkTlZP6vGyX5JD4tY/vJ+b+bRkRfTLJ3emTX1ff3BwyBacEiHliQghPoSaWYZ5hHtwVnjw5dnuZUjsy7MR8uHmjQA7ATuYO/fDaTs5jVJw73lgWIS7/xiY9nu7EnX/3OpJgKY9rQxGXiyMTFgMnaS/g5U8wMufCdwZVuzghCX6lU+5nv2kh5c6zxmBljd9MUvv5Zn/sxFymnm94YE4z51vHluvLFiTgSmLyRiJ0fO+8etQF2IlJmhBm9Pkk3kbyb2594vYT/wvDYer1cLkvVz6UPc6E24My9KAoMicRFKG8jkxCXLLMqLeDJoIBzMlZ4Nl3MLiJAehze2mWcyxEZ137NtawyDiwM1tz4dCGOxu1kMuvVqLaF+oNp1PBMqcXT+dZi7hb8KqhbKdkJ2l3ImbD37iv93IyYOAMypkvH3NXvuT2lTC4/M0JM6CIc+3XXGVb6lwwmchrX8uv4cvvg+ScUUdm8TZxeSG36JToc13NQ8vQF7niqgeWrPqcq9UCecs+ckEorukTotxbtnMnrw3vE5c3xn0q3vnXx6zZeuq92yavHgni8uLsOqTokTzXX+2yyerzYr8Wka2lJ5/WHKdc2Rs3IbdrpOqTeM4vtZPvrEjzdkOGCvlbLcap5yRh5xX9mDzMJIfu6ufWdbot/CTkZDhP0Za31+4gQnk/iCcU1PkgnCH1g1ElgI2T+dCS/Szyefl1PTXAyW+9WN8fcD9QxR1WzTAkEZmoOHgww/I+TqXyJYKLvQ9nMW750Q812eXHXb83cORoy1i2MVHsx+X4GM+MnXa39dz/CJwNyz0myZOR5dgJrAJb12ysumbrQjdI1hep5/wdjxz1cY+Jo06f8HQ+7lk8WTvXe7JK/ViewP9gXCfk16OUfp2qR569CLBllNbnfdl6OurrmWdvuk+SPqln6F+Laxt1jFcnHVvm1D3a91g/4z5m5+tD8jMn3Ffg2PicMturH8Qr95d9AQAAACDBl30BAAAAAEoGHBkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKUFjgwAAAAASgscGQAAAACUlsR/ogAAAAAAYKeDFRkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTAkQEAAABAaYEjAwAAAIDSAkcGAAAAAKUFjgwAAAAASgscGQAAAACUFjgyAAAAACgtcGQAAAAAUFrgyAAAAACgtJT6H42cmF+iqWMVvUe0fm+Ixmf0TlmZWaDlM316Z50aQ+M0q/cczs/R0pVhCp5+o0FDZ60znXJc2fhyC2g9ptrIJarr3em7yzR2UO845U/Q3NIUDesiWk9qNDJprmKmaWF5jIKn2A3t0gaOnD3ZJiFl3ptwLrf5JNFN71jifTwdCLH0ytMjt63cdsz7DMXJry+u7sb3jyS9zWUvHHm06PH1Ebp0W+7k6pvJfaZ98uuRK8dovzTo/rtp189ra0H89XzeIK1Yz+7LNSCo63bpkSG/PgXodm9652a3Z/K9stot8XiefgscSrwiM0HHq6tUGxqiId7urVPfmQWhViWGO9OZqjCe6plqT6o0tjQnnjTKxEiVVvV5Q0MNWj84RgtBBxSG6kgzlM31x1Q9s0Rz59XR+uSIvs5sNXrcEt3kgelobOhEBybRcc05VgeevquNoDzWoOaxKffeS2NUFUZQHvfuvWcQxmjqWFMYHiW/xuYwTd3N0E7R/qPGaHqwQV22HVdD2n1mxnUbhVtjgweohjaGwghr4y2P+201M0hkjrGO0DBdnI/Txk4ooC/es0b7R5re5rAXPIBcGbD6leXEZPbN9D7TNrn1yJNjpF9azIwFk5CA8/upKgdKXXex+U4MD7zLnrPDZNqTbdEjQ3v2Z3o82rekE2O1Z0PsLdn1Zp0Q8gifzXNiDoX6Vnsx4LZbR/0W+JTYkanTpbOWhzuzIrphlfaXeMCcGBmgysZiYDzrk4u0Xhmg4zHPVJ8cD42sUO8VoejVd00n82Rz+xItblRoYCTBeLBhE+alYYye3q/FGuJpGjwoZqoL5tgsNZ60qO+I7oTnj9NAZZ0WjRHMuvcuZfpIn2N4ZhceU+vgYKqjzca0ubGu90ICgyoGX59C95GOktU2evBaMe1++xGttipU7df7wqCGs9M6PXrRoko1bqbYAQX0ZeLdqpgVrwTPWl9apValGg6sqXqbZS/EAHiyT8yoLefFIrNvpt67ffK3bx9VKy1aXTJP6NsEAz+naPUN4Wn4tJpCJvEEA7NwDGKudInYk23QI0M79of7Re86rTsP5ts5JXs6NhbInvsrCYfJXsFRCKf5UMWaGGp9OTgaOFQd9VsQYdfkyEzMj1KfZWjKh1b+Z7YhZGOUwwnIVPQJ2t9L1Pw+7rg24Fan405m7zuI2VVfa5UeWXKWA4ruhMrgh4MNM/tsnSqHjos77RWUEQwHFYF0EvpoMGL0NGKGxiGlxjO9bzF7VszKUpzKvPdRhtea1elzR80sUw8CgWPjME1jx8gx7N2giL4oPQsHA//aVL31iNgLfnZ78HXI7ptF7p2fIu2r6jM8Hgyz0WsFE/MXaXhzkRpN/QdDfzW62mchV11yhYOi9sRla/TIUNz+iPpOignEgwb5Iolw+5U4xzi/LN8izsU6NYNJQof9FkQotyMjl/aWaVlsF+lmd5ZyS4PogEvq2Zc5byItdsozpKQByp89iXL397ZEhw1lu7y8B0ND28o0LXDYYr7bA6FFrLNbp0sjNVo9NKXa+UqVFj09kiEtqQOcFxG/WrFtiNn1yPVVGrii6jRVXXRCR5l6m2YveCDffEV9wfOKLSsUGLAz+gw7vBz+UPcfpaad38OI57/I4YwkO1kZpqnCz+4RsSeKHaVHFsqxa8SsqviOIZ8rnF/9W65mtoTrI8Nt+tmCUKNadeo7aYUepQ3Wv4uQOUkFTLkdGTZsOoZ4U5im5eWS58gUggchHUOdJ7ooOlJ8PJwTxzi2H+/oxM8kRQc+yc6RLv9ek4av7CXZbi/Tdzmmf3NLjTvPVKOzOk5UvCiMsW7noRUa9PRIrgZZx5wcge2GdVk67bpOzwa9Pp+ht1n24uAYDT7T13IuR6+XE5HK2+4zamIT1v8m0aTdXnrl4V7ChMfJy1DP3o4zk7QytaP0yJDh2M2e1XJgJ0VsF2nVDb0Jx2+UHWItNzvPhVew5L6+dvlIU+YNFSW+3wKfXRNakoqzYS2T7yLiQ0IWbKA5edGeAQjkLOgkJ/0mzYDU8mjcSo1jjGYaohOmhEWYlPi6RMx2MacQs/Y1/VPDuQccUrrZ1RmXfx8OjVBMmMEPx87SOOuRyXdyEMeuuzkCW0pEX2JCFmLw5T5v62VevY21Fy17JUFMFB5kh0Ttvlm4z7RNVI+iKyGi/vOivXT9TYJ+dOUhDnVtVk5XlGR7ErLNemSI6JNJdE97C8iaLIptZImoSk16FfQXd6XEz3MJnTexnX0lc5gi7eaQr9+CKLvGkSk/dXq16SfnqVhquvLHw07MaLOWHtfmfBcvnmzqkcha002wFMi4tDYU9e+bRL37HePPs7RWM9XN2WXY8XCNzD+xjSCjci+cJX1+7VLu5wlN5LyPzP9w85p2Ct3Tlwy9zYL1Wv+MktU3O7x3Inn1KAuuq/jfwXB1Qb72K/dTVo2yJic+sfZke8mtT1xX8b++M7rfLavPScj9hJUoJ/9G5ssUwMktLH+/3WmU15E5P0cL9myKkyVjktvKhMqKvxgmM/KM2Si/jO+bwU3MTu/aqy+cYyE664tH0pnIG1dNGjA4Oc55NdXOsZFvAfTRWNDZOXnPSoTkmaiYGwavV8q6lLtdiqNm87YM5eudxghyiEQOIO6MT278ZhJ/UyJXHkHGfTS2o2njJ88aPTJtOX3XHuRUaMIvu2My9EWuKkpdi8s74D4frgCk6m2WveBkS7se/Lwnwz6V2jcFqfdum7x6xL9XaF04wGPB/VR7kaz/LI3bOia22hPhtvF3UXRO1MT8XKSt1bX5SbIn26JHhlR9Evde0mGtyCvO+pVxfpU6LtQkyrnoJClzDo1tB722cdD9Klix66zfgigl/iAex/ftbxp4H68qK2yceFbO2B9JEh1pSX7jQj0jG/jgQ00C5+NV8lzRMdReiPNRJu7UnB+RIDO7HmIG4X6Iia8NP4gX/eCU3Ta7pF3awPnglf0xLSlb8mSq4WMcDoxbSUs4lngfDesK507EhhU8XbH1yCmX6dIH3qIk64vUc371XN/XrVOMbiXqbR574ep15INwdtlOX9Ik3rsz8uuR94wp7SXLtJKl/bZO/Jie1JdoUriSXbw92T49MiTpk2rfgRdxz6aOVR8kfNAuwY45djjSNqG2xX2UL7FdNan9FjiU+su+AAAAANjbIEcGAAAAAKUFjgwAAAAASgscGQAAAACUFjgyAAAAACgtcGQAAAAAUFrgyAAAAACgtMCRAQAAAEBpgSMDAAAAgNICRwYAAAAApQWODAAAAABKS+I/UfDOO+/oXwAAAADY6fz444/6194CKzIAAAAAKC1wZAAAAABQWuDIAAAAAKC0wJEBAAAAQGmBIwMAAACA0gJHBgAAAAClBY4MAAAAAEoLHBkAAAAAlBY4MgAAAAAoLXBkAAAAAFBa4MgAAAAAoLTsmn9r6ernD+lUz1O6cXqK7ui/lZJPb9HDjw7onZd0/8QFuqb3XK7SrYenyJxJ392nEx/7Z7rnvPzqBF34TO/8qkZffnKU9uld55hGyvQ9veOUf45qX16mo/riN09v0OkpW+op991DnKt9SZdDIeXSzTg9dsqRuHqR9z4dl52qj52QX19y10f2o156+uvTNPWF/psk7V7Z9UiTdd52KEr+ct36+/3S6c/0JkY2mljZpcvGbRcmvm34vAmqe/ai2+TVJ9eOSWLtqJLd4edWOY6dtjHP7dYh2m7ecYHTXpHyU9rLAv/WUpkRg/KHQQctMexcSANygk6cOEE3nvbSqS9rortFOVc7TK/1eSdO3KCnPafoy5p1JpclOgqJTqzO8Qy2cGI2zbFfP6Xej76k2q/0YdnBhdEj0an1tXbnvvq56PyvzbH7tHn0Mt36VB+U156iXtEp48veIwhDdPnopjBqSn73Xx+ly59f1QcTSNDjA737pJEL2sIeIPLepwtlp+lj+xTQl9z1EWV+EDPIpPaJ7HrwYHa5f41u6GtP2ANTO+2dh9zlevWP9MurdNjqzzeeEh29kFd2atAN5XafyJNNqh5JuAwhP8dr2AoK6JNwI3r2sYNg6iw234mROmM7gJrPLoTX6O3+d+yM3FfP/elhS17CPtNRmnDs88+oVzo94fW2c3fup73KqQqOZzsxe5ld4chc/aUYlL97qffKy7lf9NO+774OFPbO1Nf0cl8/vR/TCe9MXbAU+w5NffOS9vW/HxgmlgmJzhw7E9Gd6Lk59sW3tPZmH/X8XO9/ekp0OzGDiJmZSIP4nuj8vzHHrtH9p2/owGFtXH/1PvXve0lfm075xRR9/d0+6v9Fd4e/nc7VwwdCoya49pun9Oa9w0J6yaTp8ebvQyNnk/c+7ZStjOnzoOw7v12jN/t6nFlkxxTQl9z1Yf19/VJouEt6n8ioh3QExcCTsBrSTnvnIX+5alBe+62p3TV6LgbW3p8aOV6jC1Z/LiQ7MSgfePOU7gdy4z4vHKFfurVI0iPxFIEjxIP9llLY/mzSD0kOAjsxn/TT2q+FI/JG/y0JrR/BfYWjE+rZHfp27Q3t6/Wk/eZ1REdt3mymHQU25XdkxIyFl8vvP9f7peUcvd+/j14+D42NMkb5nABp5F//oI0sOxtWp/KRjssB+tDMEHTnN44NG8+X3yQsX0ujtkbfWp1fGkVtXJUzFg42zLXnrpO1+1HOXjioCLTMDwczZI9EPT5HP+vRPyPkvE9bZZt2/TCYzca1bacU0Zd89RGDJq9q/sZ/2PQ+kVUPPk6WQ+HSRnvnoki5ylaEzkXMtRbKqfOfJ0l2Ue78fpOo52e6jdL1iOt24URSeKe7FNEnNaFLQThBp3OuhMTL03CVTvHhYPIn+HlPENaPg1e4QH5K7sjojncrKWa8V+COYjlB3EGFt0+1L+nhw4dqc5bg79DU6Ru01n9ZHfukh74OloLZKL2h16SWVNX1ezA0tK1k6/GBj9ptiw7KZkP+6zXq/0Qdv9z7dWz+wLaRoz5XP+ewQj06+GT2iXR4YNn8/QEZHjHXh2GbncG1j0/QfTql6/chvfZzKoRDa+rO+R5+nkqi7D57Ti/3HaVTwfPGh+7a19G3yQE6pWVSRB8c/NUYDYcilTwO0/M4h0jI9LK5d0y4cN9RbZ/FttN0badRakcmsePtKcRA9VDFhJ0Zj+gkH1I9iLG6sXW+ZoLolom/PqfDTmcRM7sPiOr62hNfbdLRT26Jq8BWkK7H7HSadjJtkX+g6KhsHvguWHrw/LAwqm9RD7Lqo1ee6kkrkal9IpsDH/GApO+943K/ODdEOSjq+epEFx66eXNWXsfzw2KAtAfuVNldowvyefWgy7ZjzQ57dKajbw254hLWW+pDG85M0modO5aqbGVfk9rC5Djazkx4rdi07OHMJFNaR4az31ON1i4iMfYsZ1g884rOrjh73p4h2LH1c7UP6YCViyMN1VcvwzwXgRNa+uw+Pc1aJs+I94Zhr73MG3r9O/1TU1iPhQG8nxluVPfprGw163b1gI93Gi7JSURfMurD+QyZq7PJfSIRqx52rkp27gUTbe/uEFOuzmsL81iEc3HrKVFCSPfaxyoBVa6y5JGdM+ifpm85KJPUp3Pp6DaTw/5c+/h+Yk5iMpwSIPy6hBCeQjmCdPRUgq6ptkrURSH7up2HCCKU1JFR+STO0hy/qib3y7SsaXOHfnhtJ+cxKs4dawzZifngNd2IW7L84gfa1D+Lo+qRyO9eR5IEZVxaGwo3dq6QCYt7KnHtJb22k6cZmYfkJxa2r8fKuU27T6dlbw/d0hepg7ySqMNODx/yq61qX86EM/pEVj1ebqZle+Zt76JsVbkumbKLEJfPF2U79cjQuT4VlC23B7k5g22D5N+2Kakj4y1l8vaVaGR+V7/Er6nxDJGOTgSDjFw5MYm1PGsKBqCY2akDJ/4doFPWUiUno5kkOD9pUhylW2KQM4aJk+MOfGQv2YvZnkkGljNRu2wvP4dXb+xXDWX8ODnpcHci9PMbV4a2/NVKGh/Lo8dXqWYPJhwCeM8kZqfdp/Oy+U2LAx944YfgeJfI0BeZZyB1Lb0+d6ZOu896Qsyu5bc3zGplep/Iqgf3Gbtvuscz2rtt8uqRIC6P5cJRorVvRSkCca4dmjhXmwj6dLbsXOS1zupPmh5tM6ntqMJvgXP2ac1x6DkMe6Bgm9mTOJurn4dtZtrC1odztVrkeNBWvF8Lj/IzTPDhPWVDi7FrPognOzWvUHTpI1RvDX4O8yEk+yNKQpnVq4A8CHGH9D7kJHE/muR8AMv/0JMsT3Quvet/PMuphzBr7set3PtHPzil8nbU1W6d9hLOR8Js+UvZkidTTUSPbVky0Q+NJd7Hp9Oyt6wtk/VF6jB//0Q/U/76cJnRhNfUPpGlt06fiR7P3Q4Fya9HXnva53r9Pa6tQ3zZefbGtksS774pZbP8P9z0P6DZbZLaUT1H/5q+v2PjBIltpq7r+Sb65hU/j/OhPI2rpwKvbP+4a399+56m5y579YN4u8eRAQAAAPYw+LIvAAAAAEDJgCMDAAAAgNICRwYAAAAApQWODAAAAABKCxwZAAAAAJQWODIAAAAAKC1wZAAAAABQWuDIAAAAAKC0wJEBAAAAQGmBIwMAAACA0pL4TxQAAAAAAOx0sCIDAAAAgNICRwYAAAAApQWODAAAAABKCxwZAAAAAJQWODIAAAAAKC1wZAAAAABQWuDIAAAAAKC0wJEBAAAAQGmBIwMAAACA0gJHBgAAAAClBY4MAAAAAEpLuf+tpZkFWj7Tp3eYFj2+PkKXbuvdMuI80zo1hsZpVu+5TNPC8hgFT7/RoKGz9pkTNLc0RcMVtdd6UqORybraYc7P0dKVYdKHaf3eEI3P8C+vXIvwnIyyvTLC6/YWE/NLNHXMCOkx1UYukS2lgAw9dspJ0PHpu8s0dlDvOLrgtoXfVm7ZjKtz7vE0feyE/PqSuz5SptVEeyDl1eu2SZYsDHzeRbrp6bwiuR3ax6lXmh5ltLVTtzRbGSe7DB3NW3aa7LpHXn1y7Zgkoc34+Qaf+eWkyzuvLY8vm8nfL/Y6pV6RmXi3qhRvaEhvCR2zLLBzIQ2Iep7akyqNLc2J7hZlYn6Qmvq8oaEaPe4do6X58Mzpu6KDbhrZNKh5bIoWgk4gOohwYpqiY8jj1x9T9cwSzZ3nY7M0Lq+xtnvropc+poa+Pr1sNg5jVBWdOlr2HkIYsaljTWG8lAwbm8M0dXdaH3RJ1+NpGqsuhsfuNWn4yoL4q4HlLQYRsq4PDLHXFpG2IuqrVqQBDq61ja33DGn62D4F9CV3fUSZJ+1B10P0s9Fg0A1JlYWEB5Zlz9kxpLVDB+TWo6y2nqZBq261J0TDk/lll6Wj2WWnya6bFNAn4SJUK+x0mWcSm99mbJNFvUMnzZAh7zy2PLFsgTw2RmRstNjgxCRT+tBSqykG2V3CxMgAVTYWAwNRn1yk9coAHY/phPXJccuQ1OnSg3WqHDquO4owLAdFB10wnXKWGk9a1HdEG8Dz+6kqZggrpmPcfkSrrQpV+/W+gzJs6w/MLDCr7OM0UFmnRTMzuX2JFjcqNDASNZm7mekjfWJQbAQD4ezCY2odHBTSiydZj4VjaRvXmQY9bvXRoGm7mTEaJjFLjx00laFeXTKzxFla2SCqvuu2RfP7+NmxGrxWgmeoL61Sq1INZohdoYC+5K4Py2RzXWh4PNPjwonfiD+aJAs1EKuBpSFkGCG1Hdonvx5ltbWrR+3ILq+ORsvOkF03KWx/mvQqafLLzsSVAVq9LiaKLf23gHR5Z9ry1LKVjpJwkuC85KPUjgzPoHYPE3T8UIXWn9mGkDtHPidAGvnNV8rZmBmkvtYqPbI6qDQuxgBKx6WPRs0Kju78gWNjow20WY3JKlt14HCwYWaf2U7WXkA5e6GRE2iZBw6IRTE9Vga0uab2eKALnUwfpT/D42bY8+s1Qft79c8YVLuOBrPZuLbtlCL6kq8+YtDkmfDCit73mFmQIaXGM70fkC4LliWvVCYNLOnt0C5F9CirrV3UQBk6SIpk2RXR0WjZ6bLrJoXsj5zQpSCcoJHEVf40eeew5allc1mWMwYyKf2KTOXYFC0vL8vNXjLfW0zT2DG/46RRp0sjNVo9pGV3pUqLsfFbfzUGbBV59XhiftRyJHnwFU4NqSVqdb27jD57VsyCaUwfG6VmTO5C35n4a6Wxvb5KA1fU8SkOcXV5xaEQOeozfZeX+28mDhByoJ5P1udEWaSS3Q7bQWZbCydOHVM5GX6eSrrsMnQ0o+ydSx+N6XovFwyb5ulbbcEOVqtJNL8UyLRo3fYapXZkWJFM/FDFQveiM6OWbTlWm3/Gw9dcFB3FyG+FBkVniciOV2rs1RiwJeTVY5X02aSGk+wpZoUniW6a650cGo7jq4FFlX+TaHLZyqVih9YcM9daAzAPTpNW2c8GhVG183O2maz66NWWmwmDaPpAnSGLTNLaYTvIamvBzHjwfCtHvMExQ3aZOppW9k5FroqEzyXzj3LXO4e8O6EyTKN0061bQo4d2AUrMgFCKW/auRq7iMS4vZwF8Uwg5wxIePkc5Zazeit+K8wUjd9bj8iOl2npxaN8qzG67ERM2GtPE4aEEonVY2U05QpEzMqZs2Jm59D4YUEerOcfEyWF+cRg1AiWv2NW4+Tx+PBY14noS0Z9THJlwmoLO4FpA3UERxb5SGyHrhOjRwXbevZsTZw9TGM5ZBchw9Y6Ze8Uctif2bONxJzECEX7liY5B8vHDS1l5djtdXaPI6Mpb/JvnV5t+omYKu4aO/ixE3OySbW4OOtaM5LIJ2PHhZwJjvNSNMaeUXb9+yZR736nM8uExV2UlJ3NOjX95GmZh5SSWOgRyoudmCmqPhAzs0hYR+lMt8lvbDunW/oidZBXRXTYaVm+tqr2l+b/XOYs8Cx3yizV82uxcj991SWfLLamHbqhR3lIl13ysLwT+3Tn+tQN2Ra05T63X4lagCKU2JERBn7e8k/FrOLisZiBt0Sw103HLgaG1cmH4FlTYHRjZqc2Ysa0KGaqY8FSpJtD4ydN8vEFYdidHJskg5lRtpyJirnKxSCRmF91TU463J2I2dmDdeo7E4YWOAEySEKUK2lW+CdNjyMzPxdOZLTvI883idszK2KGac+Mxb0mh61Vtmlxb8vYcnjhoEn6rtOjF2LWfdILPwTHu0SGvvA3NpalrqXXpz45EizDq03MruX3THi18k/dsBFv+pMC4UQgTRbZpLZD2xTQo6y2FufaoaCJ+YtB/dJlx1dn6Why2dtOqj6J51iynLOZOceJ5dBjXyR5PIEMeafa8kw4Mdi2sV67gwgl/iAeK6X9MSPueF1KtnqbsHHi2SJjf/xKdEj1uh4/o//sBlsG7jmRjynJ8sIP4sV+zOnISkJyZ0bZwrRy3o56il3SLm3gfMjM/tCWbGPSH8jK0GNbH2wi5Zlz/A9v2W0hsK/zj0Wu9Z5hy9oyWV/kh9b4GyW6zvnrw2WmJGCyzHhFM8g3ypaFges02vQ/AilIbYf2yadHTEpbe/09vX6+7DJ0tEDZibLrKkn6pJ5j4IW+v9+3nL5ho67jVdFkOyfwr7fLT/yQYVLZSlbd/rjibqXcX/YFAAAAwJ5m1+XIAAAAAGDvAEcGAAAAAKUFjgwAAAAASgscGQAAAACUFjgyAAAAACgtcGQAAAAAUFrgyAAAAACgtMCRAQAAAEBpgSMDAAAAgNICRwYAAAAApQWODAAAAABKCxwZAAAAAJQWODIAAAAAKClE/x/ULhj9G0wJtgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "e68c4a58",
   "metadata": {},
   "source": [
    "Using device: cuda\n",
    "Transformers version: 4.56.1\n",
    "Loading and cleaning datasets...\n",
    "\n",
    "üßπ Cleaning Train dataset...\n",
    "  Original size: 42380\n",
    "  Columns: ['Headline', 'Content', 'Label']\n",
    "  Missing values in Headline: 0\n",
    "  Missing values in Content: 1\n",
    "  Missing labels: 0\n",
    "  Final size: 42022 (358 removed)\n",
    "  Sample text type: <class 'str'>\n",
    "  Sample text preview: '‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶Ü‡¶ü ‡¶¨‡¶õ‡¶∞‡ßá...'\n",
    "\n",
    "üßπ Cleaning Validation dataset...\n",
    "  Original size: 9082\n",
    "  Columns: ['Headline', 'Content', 'Label']\n",
    "  Missing values in Headline: 0\n",
    "  Missing values in Content: 0\n",
    "  Missing labels: 0\n",
    "  Final size: 9062 (20 removed)\n",
    "  Sample text type: <class 'str'>\n",
    "  Sample text preview: '‡¶∏‡ßã‡¶®‡¶æ‡¶∞‡¶ó‡¶æ‡¶Å‡ßü‡ßá ‡¶¶‡ßÅ‡¶á ‡¶Ø‡ßÅ‡¶¨‡¶ï ‡¶ñ‡ßÅ‡¶® , ‡¶Ü‡¶ü‡¶ï ‡ß© ‡¶∏‡ßç‡¶ü‡¶æ‡¶´ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü‡¶æ‡¶∞, ‡¶®‡¶æ‡¶∞‡¶æ‡ßü‡¶£‡¶ó‡¶û‡ßç‡¶ú ‡•• ‡¶∏‡ßã‡¶®‡¶æ‡¶∞‡¶ó‡¶æ‡¶Å‡¶ì ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶ö‡¶Å‡¶™‡ßÅ‡¶∞‡ßá ‡¶∏‡ßã‡¶®‡¶æ‡¶™‡ßÅ‡¶∞ ‡¶è‡¶≤‡¶æ...'\n",
    "\n",
    "üßπ Cleaning Test dataset...\n",
    "  Original size: 9082\n",
    "  Columns: ['Headline', 'Content', 'Label']\n",
    "  Missing values in Headline: 0\n",
    "  Missing values in Content: 0\n",
    "  Missing labels: 0\n",
    "  Final size: 9061 (21 removed)\n",
    "  Sample text type: <class 'str'>\n",
    "  Sample text preview: '‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø‡¶§‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ‡¶∞ ‡¶Ö‡¶¨‡¶¶‡¶æ‡¶® ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß¶.‡ßØ ‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂ ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶ñ‡¶æ‡¶§‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶π‡¶≤‡ßá‡¶ì ‡¶Æ‡ßã‡¶ü ‡¶¶‡ßá‡¶∂‡¶ú ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶®‡ßá (...'\n",
    "\n",
    "üìä Final dataset sizes:\n",
    "  Train: 42,022 samples\n",
    "  Validation: 9,062 samples\n",
    "  Test: 9,061 samples\n",
    "\n",
    "üìà Label distribution in training set:\n",
    "  Class 0: 1,376 samples (3.3%)\n",
    "  Class 1: 2,081 samples (5.0%)\n",
    "  Class 2: 4,475 samples (10.6%)\n",
    "  Class 3: 34,090 samples (81.1%)\n",
    "\n",
    "ü§ñ Loading model: sagorsarker/bangla-bert-base\n",
    "\n",
    "üóÇÔ∏è Creating datasets...\n",
    "üîß Creating dataset from 42022 samples...\n",
    "‚úÖ Dataset ready: 42022 valid samples (0 skipped)\n",
    "üìù Sample text: '‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶Ü‡¶ü ‡¶¨‡¶õ‡¶∞‡ßá...'\n",
    "üè∑Ô∏è  Sample label: 3\n",
    "üîß Creating dataset from 9062 samples...\n",
    "‚úÖ Dataset ready: 9062 valid samples (0 skipped)\n",
    "üìù Sample text: '‡¶∏‡ßã‡¶®‡¶æ‡¶∞‡¶ó‡¶æ‡¶Å‡ßü‡ßá ‡¶¶‡ßÅ‡¶á ‡¶Ø‡ßÅ‡¶¨‡¶ï ‡¶ñ‡ßÅ‡¶® , ‡¶Ü‡¶ü‡¶ï ‡ß© ‡¶∏‡ßç‡¶ü‡¶æ‡¶´ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü‡¶æ‡¶∞, ‡¶®‡¶æ‡¶∞‡¶æ‡ßü‡¶£‡¶ó‡¶û‡ßç‡¶ú ‡•• ‡¶∏‡ßã‡¶®‡¶æ‡¶∞‡¶ó‡¶æ‡¶Å‡¶ì ‡¶â‡¶™‡¶ú‡ßá‡¶≤‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶ö‡¶Å‡¶™‡ßÅ‡¶∞‡ßá ‡¶∏‡ßã‡¶®‡¶æ‡¶™‡ßÅ‡¶∞ ‡¶è‡¶≤‡¶æ...'\n",
    "üè∑Ô∏è  Sample label: 3\n",
    "üîß Creating dataset from 9061 samples...\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "‚úÖ Dataset ready: 9061 valid samples (0 skipped)\n",
    "üìù Sample text: '‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø‡¶§‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ‡¶∞ ‡¶Ö‡¶¨‡¶¶‡¶æ‡¶® ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß¶.‡ßØ ‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂ ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶ñ‡¶æ‡¶§‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶π‡¶≤‡ßá‡¶ì ‡¶Æ‡ßã‡¶ü ‡¶¶‡ßá‡¶∂‡¶ú ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶®‡ßá (...'\n",
    "üè∑Ô∏è  Sample label: 3\n",
    "\n",
    "üìö High class imbalance detected (24.8:1)\n",
    "   Recommending 5 epochs\n",
    "\n",
    "‚öôÔ∏è Training configuration:\n",
    "  Batch size: 16\n",
    "  Steps per epoch: 2,626\n",
    "  Total epochs: 5\n",
    "  Total steps: 13,130\n",
    "  Estimated time: ~125-225 minutes\n",
    "\n",
    "üöÄ Initializing trainer...\n",
    "\n",
    "üîç Final validation:\n",
    "  Train dataset: 42,022 samples\n",
    "  Val dataset: 9,062 samples\n",
    "  Model device: cuda:0\n",
    "\n",
    "üß™ Testing one batch...\n",
    "‚úÖ Batch test successful!\n",
    "\n",
    "================================================================================\n",
    "üéØ STARTING BULLETPROOF TRAINING\n",
    "================================================================================\n",
    "üìä Dataset: BangFakeNews (42,022 samples)\n",
    "ü§ñ Model: sagorsarker/bangla-bert-base\n",
    "üìö Epochs: 5 (with early stopping)\n",
    "‚è±Ô∏è  Estimated time: ~150 minutes\n",
    "üìà Progress logged every 100 steps\n",
    "================================================================================\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn( \n",
    "\n",
    "================================================================================\n",
    "üéâ TRAINING COMPLETED SUCCESSFULLY!\n",
    "================================================================================\n",
    "üìà Final training loss: 0.3562\n",
    "\n",
    "üìä Final validation evaluation...\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "  eval_loss: 0.4108\n",
    "  eval_accuracy: 0.8636\n",
    "  eval_f1: 0.8452\n",
    "  eval_f1_macro: 0.4969\n",
    "  eval_runtime: 101.7551\n",
    "  eval_samples_per_second: 89.0570\n",
    "  eval_steps_per_second: 1.3960\n",
    "\n",
    "üéØ Test set evaluation...\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "üèÜ FINAL RESULTS:\n",
    "  Test Accuracy: 0.8627\n",
    "  Test F1-Score: 0.8448\n",
    "\n",
    "üíæ Saving final model...\n",
    "\n",
    "üéä SUCCESS! Model saved to './final-bangla-fake-news-model'\n",
    "\n",
    "================================================================================\n",
    "üèÅ TRAINING SESSION COMPLETE\n",
    "================================================================================\n",
    "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883433be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Discourse-aware Psycholinguistic Framework for Bangla Fake News Detection\n",
    "# Implementation based on the research paper approach\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class BanglaPsycholinguisticFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract psycholinguistic markers from Bangla text\n",
    "    Based on established deception detection literature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Bangla linguistic markers for deception detection\n",
    "        self.emotional_words = self._load_emotional_lexicon()\n",
    "        self.formal_markers = self._load_formal_markers()\n",
    "        self.uncertainty_markers = self._load_uncertainty_markers()\n",
    "        self.cognitive_load_markers = self._load_cognitive_load_markers()\n",
    "        \n",
    "    def _load_emotional_lexicon(self):\n",
    "        \"\"\"Load Bangla emotional word lexicon\"\"\"\n",
    "        # High-arousal emotional words often indicate deception\n",
    "        emotional_words = {\n",
    "            'positive': ['‡¶ö‡¶Æ‡ßé‡¶ï‡¶æ‡¶∞', '‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£', '‡¶¶‡ßÅ‡¶∞‡ßç‡¶¶‡¶æ‡¶®‡ßç‡¶§', '‡¶¨‡¶ø‡¶∏‡ßç‡¶Æ‡¶Ø‡¶º‡¶ï‡¶∞', '‡¶Ö‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶Ø'],\n",
    "            'negative': ['‡¶≠‡¶Ø‡¶º‡¶æ‡¶®‡¶ï', '‡¶¶‡ßÅ‡¶∞‡ßç‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶ú‡¶®‡¶ï', '‡¶¨‡¶ø‡¶™‡¶ú‡ßç‡¶ú‡¶®‡¶ï', '‡¶ï‡ßç‡¶∑‡¶§‡¶ø‡¶ï‡¶∞', '‡¶Æ‡¶æ‡¶∞‡¶æ‡¶§‡ßç‡¶Æ‡¶ï'],\n",
    "            'fear': ['‡¶≠‡¶Ø‡¶º', '‡¶Ü‡¶§‡¶ô‡ßç‡¶ï', '‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶æ', '‡¶¶‡ßÅ‡¶∂‡ßç‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶æ', '‡¶Ü‡¶∂‡¶ô‡ßç‡¶ï‡¶æ'],\n",
    "            'anger': ['‡¶∞‡¶æ‡¶ó', '‡¶ï‡ßç‡¶∞‡ßã‡¶ß', '‡¶¨‡¶ø‡¶∞‡¶ï‡ßç‡¶§‡¶ø', '‡¶ò‡ßÉ‡¶£‡¶æ', '‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∂‡ßã‡¶ß']\n",
    "        }\n",
    "        return emotional_words\n",
    "    \n",
    "    def _load_formal_markers(self):\n",
    "        \"\"\"Load formal language markers\"\"\"\n",
    "        return {\n",
    "            'formal_pronouns': ['‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶â‡¶®‡¶ø', '‡¶§‡¶ø‡¶®‡¶ø'],\n",
    "            'informal_pronouns': ['‡¶Ü‡¶Æ‡¶ø', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶∞‡¶æ', '‡¶∏‡ßá', '‡¶ì‡¶∞‡¶æ'],\n",
    "            'formal_verbs': ['‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®', '‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®'],\n",
    "            'passive_constructions': ['‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá']\n",
    "        }\n",
    "    \n",
    "    def _load_uncertainty_markers(self):\n",
    "        \"\"\"Load uncertainty and hedging markers\"\"\"\n",
    "        return {\n",
    "            'hedging': ['‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶§', '‡¶π‡¶Ø‡¶º‡¶§‡ßã', '‡¶Æ‡¶®‡ßá ‡¶π‡¶Ø‡¶º', '‡¶¨‡¶≤‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º', '‡¶ß‡¶æ‡¶∞‡¶£‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º'],\n",
    "            'uncertainty': ['‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶®‡¶Ø‡¶º', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶®‡¶Ø‡¶º', '‡¶Ö‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü', '‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π‡¶ú‡¶®‡¶ï'],\n",
    "            'qualification': ['‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ', '‡¶Ö‡¶®‡ßá‡¶ï‡¶ü‡¶æ', '‡¶Æ‡ßã‡¶ü‡¶æ‡¶Æ‡ßÅ‡¶ü‡¶ø', '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º']\n",
    "        }\n",
    "    \n",
    "    def _load_cognitive_load_markers(self):\n",
    "        \"\"\"Load cognitive load indicators\"\"\"\n",
    "        return {\n",
    "            'complexity': ['‡¶ú‡¶ü‡¶ø‡¶≤', '‡¶ï‡¶†‡¶ø‡¶®', '‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§', '‡¶¨‡¶ø‡¶∂‡¶¶'],\n",
    "            'simplification': ['‡¶∏‡¶π‡¶ú', '‡¶∏‡¶∞‡¶≤', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü', '‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞'],\n",
    "            'repetition_markers': ['‡¶Ü‡¶¨‡¶æ‡¶∞', '‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡¶Ø‡¶º', '‡¶Ü‡¶∞‡ßá‡¶ï‡¶¨‡¶æ‡¶∞', '‡¶´‡¶ø‡¶∞‡ßá']\n",
    "        }\n",
    "    \n",
    "    def extract_psycholinguistic_features(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract comprehensive psycholinguistic features\"\"\"\n",
    "        features = {}\n",
    "        words = text.split()\n",
    "        total_words = len(words)\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return {f'psycho_{key}': 0.0 for key in range(20)}\n",
    "        \n",
    "        # 1. Emotional intensity features\n",
    "        features.update(self._extract_emotional_features(text, words, total_words))\n",
    "        \n",
    "        # 2. Linguistic complexity features\n",
    "        features.update(self._extract_complexity_features(text, words, total_words))\n",
    "        \n",
    "        # 3. Uncertainty and hedging features\n",
    "        features.update(self._extract_uncertainty_features(text, words, total_words))\n",
    "        \n",
    "        # 4. Cognitive load indicators\n",
    "        features.update(self._extract_cognitive_load_features(text, words, total_words))\n",
    "        \n",
    "        # 5. Deception-specific markers\n",
    "        features.update(self._extract_deception_markers(text, words, total_words))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_emotional_features(self, text, words, total_words):\n",
    "        \"\"\"Extract emotional markers linked to deception\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Count emotional words by category\n",
    "        for emotion, word_list in self.emotional_words.items():\n",
    "            count = sum(1 for word in words if any(emo_word in word for emo_word in word_list))\n",
    "            features[f'psycho_emotion_{emotion}'] = count / total_words\n",
    "        \n",
    "        # Emotional intensity (exclamation marks, caps)\n",
    "        features['psycho_exclamation_ratio'] = text.count('!') / max(len(text), 1)\n",
    "        features['psycho_caps_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_complexity_features(self, text, words, total_words):\n",
    "        \"\"\"Extract linguistic complexity indicators\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Sentence complexity\n",
    "        sentences = re.split(r'[‡•§!?]', text)\n",
    "        features['psycho_avg_sentence_length'] = np.mean([len(s.split()) for s in sentences if s.strip()])\n",
    "        \n",
    "        # Word complexity (average word length)\n",
    "        features['psycho_avg_word_length'] = np.mean([len(word) for word in words])\n",
    "        \n",
    "        # Vocabulary richness (type-token ratio)\n",
    "        unique_words = len(set(words))\n",
    "        features['psycho_vocabulary_richness'] = unique_words / total_words\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_uncertainty_features(self, text, words, total_words):\n",
    "        \"\"\"Extract uncertainty and hedging markers\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        for marker_type, word_list in self.uncertainty_markers.items():\n",
    "            count = sum(1 for word in words if any(marker in word for marker in word_list))\n",
    "            features[f'psycho_{marker_type}'] = count / total_words\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_cognitive_load_features(self, text, words, total_words):\n",
    "        \"\"\"Extract cognitive load indicators\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Repetition (potential sign of cognitive load)\n",
    "        word_counts = Counter(words)\n",
    "        repeated_words = sum(1 for count in word_counts.values() if count > 1)\n",
    "        features['psycho_repetition_ratio'] = repeated_words / total_words\n",
    "        \n",
    "        # Disfluency markers (‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé, ‡¶Æ‡¶æ‡¶®‡ßá, etc.)\n",
    "        disfluency_markers = ['‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé', '‡¶Æ‡¶æ‡¶®‡ßá', '‡¶Ø‡ßá‡¶Æ‡¶®', '‡¶Ü‡¶∏‡¶≤‡ßá']\n",
    "        disfluency_count = sum(1 for word in words if word in disfluency_markers)\n",
    "        features['psycho_disfluency_ratio'] = disfluency_count / total_words\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_deception_markers(self, text, words, total_words):\n",
    "        \"\"\"Extract specific deception indicators from literature\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Self-referencing (deceptive texts often have fewer self-references)\n",
    "        self_refs = ['‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞']\n",
    "        self_ref_count = sum(1 for word in words if word in self_refs)\n",
    "        features['psycho_self_reference'] = self_ref_count / total_words\n",
    "        \n",
    "        # Other-referencing (deceptive texts may have more other-references)\n",
    "        other_refs = ['‡¶∏‡ßá', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶§‡¶æ‡¶ï‡ßá']\n",
    "        other_ref_count = sum(1 for word in words if word in other_refs)\n",
    "        features['psycho_other_reference'] = other_ref_count / total_words\n",
    "        \n",
    "        # Present vs past tense (deceptive texts may avoid present tense)\n",
    "        present_markers = ['‡¶Ü‡¶õ‡¶ø', '‡¶Ü‡¶õ‡ßá‡¶®', '‡¶Ü‡¶õ‡ßá', '‡¶ï‡¶∞‡¶õ‡¶ø', '‡¶ï‡¶∞‡¶õ‡ßá‡¶®']\n",
    "        present_count = sum(1 for word in words if any(marker in word for marker in present_markers))\n",
    "        features['psycho_present_tense'] = present_count / total_words\n",
    "        \n",
    "        return features\n",
    "\n",
    "class DiscourseAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze discourse structure and coherence patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "    \n",
    "    def extract_discourse_features(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract discourse-level features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Split into discourse segments (paragraphs)\n",
    "        paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "        \n",
    "        if len(paragraphs) == 0:\n",
    "            return {f'discourse_{key}': 0.0 for key in range(10)}\n",
    "        \n",
    "        # 1. Coherence analysis\n",
    "        features.update(self._analyze_coherence(paragraphs))\n",
    "        \n",
    "        # 2. Topic progression\n",
    "        features.update(self._analyze_topic_progression(paragraphs))\n",
    "        \n",
    "        # 3. Argumentative structure\n",
    "        features.update(self._analyze_argumentative_structure(text, paragraphs))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _analyze_coherence(self, paragraphs):\n",
    "        \"\"\"Analyze semantic coherence across paragraphs\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if len(paragraphs) < 2:\n",
    "            features['discourse_coherence_score'] = 1.0\n",
    "            return features\n",
    "        \n",
    "        # Get embeddings for each paragraph\n",
    "        embeddings = []\n",
    "        for paragraph in paragraphs:\n",
    "            inputs = self.tokenizer(paragraph, return_tensors='pt', \n",
    "                                  padding=True, truncation=True, max_length=256)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "                embeddings.append(embedding)\n",
    "        \n",
    "        # Calculate coherence as average cosine similarity between adjacent paragraphs\n",
    "        coherence_scores = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            similarity = torch.cosine_similarity(embeddings[i], embeddings[i+1], dim=0)\n",
    "            coherence_scores.append(similarity.item())\n",
    "        \n",
    "        features['discourse_coherence_score'] = np.mean(coherence_scores)\n",
    "        features['discourse_coherence_variance'] = np.var(coherence_scores)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _analyze_topic_progression(self, paragraphs):\n",
    "        \"\"\"Analyze topic progression patterns\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Topic shift indicators\n",
    "        transition_words = ['‡¶§‡¶¨‡ßá', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá', '‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ', '‡¶Ü‡¶¨‡¶æ‡¶∞', '‡¶è‡¶∞‡¶™‡¶∞']\n",
    "        \n",
    "        total_transitions = 0\n",
    "        for paragraph in paragraphs:\n",
    "            for word in transition_words:\n",
    "                total_transitions += paragraph.count(word)\n",
    "        \n",
    "        features['discourse_topic_transitions'] = total_transitions / len(paragraphs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _analyze_argumentative_structure(self, text, paragraphs):\n",
    "        \"\"\"Analyze argumentative and rhetorical structure\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Claim markers\n",
    "        claim_markers = ['‡¶¶‡¶æ‡¶¨‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º', '‡¶Ö‡¶≠‡¶ø‡¶Ø‡ßã‡¶ó', '‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º', '‡¶ú‡¶æ‡¶®‡¶æ ‡¶ó‡ßá‡¶õ‡ßá']\n",
    "        evidence_markers = ['‡¶™‡ßç‡¶∞‡¶Æ‡¶æ‡¶£', '‡¶§‡¶•‡ßç‡¶Ø', '‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£', '‡¶™‡¶∞‡¶ø‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶®']\n",
    "        \n",
    "        claim_count = sum(text.count(marker) for marker in claim_markers)\n",
    "        evidence_count = sum(text.count(marker) for marker in evidence_markers)\n",
    "        \n",
    "        features['discourse_claim_ratio'] = claim_count / len(paragraphs)\n",
    "        features['discourse_evidence_ratio'] = evidence_count / len(paragraphs)\n",
    "        features['discourse_claim_evidence_balance'] = evidence_count / max(claim_count, 1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class IntegratedDiscourseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrated model combining BERT embeddings with discourse and psycholinguistic features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_labels=4, psycho_features=20, discourse_features=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Feature dimensions\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        total_features = bert_hidden_size + psycho_features + discourse_features\n",
    "        \n",
    "        # Classification layers\n",
    "        self.feature_fusion = nn.Linear(total_features, bert_hidden_size)\n",
    "        self.classifier = nn.Linear(bert_hidden_size, num_labels)\n",
    "        \n",
    "        # Feature importance (for interpretability)\n",
    "        self.feature_attention = nn.Linear(total_features, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, psycho_features, discourse_features):\n",
    "        # Get BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled = bert_outputs.pooler_output\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            bert_pooled, \n",
    "            psycho_features, \n",
    "            discourse_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Feature fusion with attention\n",
    "        attention_weights = torch.softmax(self.feature_attention(combined_features), dim=1)\n",
    "        weighted_features = combined_features * attention_weights\n",
    "        \n",
    "        # Final classification\n",
    "        fused_features = self.dropout(self.feature_fusion(weighted_features))\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'attention_weights': attention_weights,\n",
    "            'bert_features': bert_pooled,\n",
    "            'psycho_features': psycho_features,\n",
    "            'discourse_features': discourse_features\n",
    "        }\n",
    "\n",
    "class InterpretabilityAnalyzer:\n",
    "    \"\"\"\n",
    "    Provide interpretable explanations for model decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, psycho_extractor, discourse_analyzer):\n",
    "        self.model = model\n",
    "        self.psycho_extractor = psycho_extractor\n",
    "        self.discourse_analyzer = discourse_analyzer\n",
    "        \n",
    "    def explain_prediction(self, text: str, prediction: int, confidence: float):\n",
    "        \"\"\"Provide human-readable explanation for prediction\"\"\"\n",
    "        \n",
    "        # Extract features\n",
    "        psycho_features = self.psycho_extractor.extract_psycholinguistic_features(text)\n",
    "        discourse_features = self.discourse_analyzer.extract_discourse_features(text)\n",
    "        \n",
    "        explanation = {\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'psycholinguistic_indicators': self._interpret_psycho_features(psycho_features),\n",
    "            'discourse_indicators': self._interpret_discourse_features(discourse_features),\n",
    "            'decision_factors': self._rank_decision_factors(psycho_features, discourse_features)\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _interpret_psycho_features(self, features):\n",
    "        \"\"\"Interpret psycholinguistic features\"\"\"\n",
    "        indicators = []\n",
    "        \n",
    "        # High emotional intensity\n",
    "        if features.get('psycho_emotion_negative', 0) > 0.05:\n",
    "            indicators.append(\"High negative emotional language detected\")\n",
    "        \n",
    "        # Uncertainty markers\n",
    "        if features.get('psycho_hedging', 0) > 0.03:\n",
    "            indicators.append(\"Excessive hedging language (uncertainty)\")\n",
    "        \n",
    "        # Cognitive load\n",
    "        if features.get('psycho_repetition_ratio', 0) > 0.3:\n",
    "            indicators.append(\"High repetition (potential cognitive load)\")\n",
    "        \n",
    "        # Self-reference patterns\n",
    "        if features.get('psycho_self_reference', 0) < 0.01:\n",
    "            indicators.append(\"Unusually low self-reference (deception marker)\")\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def _interpret_discourse_features(self, features):\n",
    "        \"\"\"Interpret discourse-level features\"\"\"\n",
    "        indicators = []\n",
    "        \n",
    "        # Coherence issues\n",
    "        if features.get('discourse_coherence_score', 1.0) < 0.5:\n",
    "            indicators.append(\"Low discourse coherence detected\")\n",
    "        \n",
    "        # Claim-evidence imbalance\n",
    "        if features.get('discourse_claim_evidence_balance', 1.0) < 0.3:\n",
    "            indicators.append(\"Claims not supported by evidence\")\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def _rank_decision_factors(self, psycho_features, discourse_features):\n",
    "        \"\"\"Rank the most important decision factors\"\"\"\n",
    "        # This would be implemented based on feature importance from the model\n",
    "        factors = []\n",
    "        \n",
    "        all_features = {**psycho_features, **discourse_features}\n",
    "        \n",
    "        # Sort features by absolute value (importance)\n",
    "        sorted_features = sorted(all_features.items(), \n",
    "                               key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        for feature, value in sorted_features[:5]:\n",
    "            factors.append({\n",
    "                'feature': feature,\n",
    "                'value': value,\n",
    "                'interpretation': self._interpret_single_feature(feature, value)\n",
    "            })\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def _interpret_single_feature(self, feature, value):\n",
    "        \"\"\"Provide interpretation for individual feature\"\"\"\n",
    "        interpretations = {\n",
    "            'psycho_emotion_negative': 'Negative emotional language intensity',\n",
    "            'psycho_hedging': 'Use of uncertain/hedging language',\n",
    "            'discourse_coherence_score': 'Overall text coherence',\n",
    "            'psycho_self_reference': 'Self-referential language use',\n",
    "            'discourse_claim_evidence_balance': 'Balance between claims and evidence'\n",
    "        }\n",
    "        \n",
    "        return interpretations.get(feature, f\"Feature {feature}\")\n",
    "\n",
    "# Example usage and evaluation\n",
    "def demonstrate_framework():\n",
    "    \"\"\"Demonstrate the discourse-aware psycholinguistic framework\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "    bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "    \n",
    "    psycho_extractor = BanglaPsycholinguisticFeatureExtractor()\n",
    "    discourse_analyzer = DiscourseAnalyzer(tokenizer, bert_model)\n",
    "    \n",
    "    # Example text analysis\n",
    "    sample_text = \"\"\"\n",
    "    ‡¶ö‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßç‡¶Ø‡¶ï‡¶∞ ‡¶ñ‡¶¨‡¶∞! ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶®‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶®‡¶æ‡¶ï‡¶ø ‡¶ó‡ßã‡¶™‡¶® ‡¶∏‡¶≠‡¶æ‡¶Ø‡¶º ‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶® ‡¶Ø‡ßá ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø ‡¶≠‡ßá‡¶ô‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§ \n",
    "    ‡¶è‡¶á ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶®‡¶Ø‡¶º ‡¶§‡¶¨‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶®‡ßá ‡¶π‡¶Ø‡¶º ‡¶è‡¶ü‡¶ø ‡¶∏‡¶§‡ßç‡¶Ø ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract features\n",
    "    psycho_features = psycho_extractor.extract_psycholinguistic_features(sample_text)\n",
    "    discourse_features = discourse_analyzer.extract_discourse_features(sample_text)\n",
    "    \n",
    "    print(\"Psycholinguistic Features:\")\n",
    "    for key, value in psycho_features.items():\n",
    "        if value > 0:\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nDiscourse Features:\")\n",
    "    for key, value in discourse_features.items():\n",
    "        if value > 0:\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    return psycho_features, discourse_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01068e1",
   "metadata": {},
   "source": [
    "Psycholinguistic Features:\n",
    "  psycho_exclamation_ratio: 0.0056\n",
    "  psycho_avg_sentence_length: 6.7500\n",
    "  psycho_avg_word_length: 5.0741\n",
    "  psycho_vocabulary_richness: 1.0000\n",
    "  psycho_self_reference: 0.0370\n",
    "\n",
    "Discourse Features:\n",
    "  discourse_coherence_score: 0.6363\n",
    "  discourse_topic_transitions: 0.5000\n",
    "  discourse_evidence_ratio: 0.5000\n",
    "  discourse_claim_evidence_balance: 1.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031562",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/train_cleaned.csv\")\n",
    "val_df = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/val_cleaned.csv\") \n",
    "test_df = pd.read_csv(\"/kaggle/input/banfakenews-2-0-bangla-fake-news-dataset/test_cleaned.csv\")\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['combined_text'] = df['Headline'].astype(str) + ' ' + df['Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e55afd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "bert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "psycho_extractor = BanglaPsycholinguisticFeatureExtractor()\n",
    "discourse_analyzer = DiscourseAnalyzer(tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e614d8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fixed feature extraction with consistent feature vectors\n",
    "def extract_all_features_fixed(df, psycho_extractor, discourse_analyzer):\n",
    "    psycho_features_list = []\n",
    "    discourse_features_list = []\n",
    "    \n",
    "    # First, determine the expected feature dimensions by testing one sample\n",
    "    sample_text = df.iloc[0]['combined_text']\n",
    "    sample_psycho = psycho_extractor.extract_psycholinguistic_features(sample_text)\n",
    "    sample_discourse = discourse_analyzer.extract_discourse_features(sample_text)\n",
    "    \n",
    "    psycho_feature_names = list(sample_psycho.keys())\n",
    "    discourse_feature_names = list(sample_discourse.keys())\n",
    "    \n",
    "    print(f\"Expected psycholinguistic features: {len(psycho_feature_names)}\")\n",
    "    print(f\"Expected discourse features: {len(discourse_feature_names)}\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx}/{len(df)} samples\")\n",
    "        \n",
    "        text = row['combined_text']\n",
    "        \n",
    "        try:\n",
    "            # Extract features\n",
    "            psycho_features = psycho_extractor.extract_psycholinguistic_features(text)\n",
    "            discourse_features = discourse_analyzer.extract_discourse_features(text)\n",
    "            \n",
    "            # Ensure consistent feature ordering and handle missing features\n",
    "            psycho_vector = []\n",
    "            for feature_name in psycho_feature_names:\n",
    "                psycho_vector.append(psycho_features.get(feature_name, 0.0))\n",
    "            \n",
    "            discourse_vector = []\n",
    "            for feature_name in discourse_feature_names:\n",
    "                discourse_vector.append(discourse_features.get(feature_name, 0.0))\n",
    "            \n",
    "            psycho_features_list.append(psycho_vector)\n",
    "            discourse_features_list.append(discourse_vector)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Add zero vectors for failed samples\n",
    "            psycho_features_list.append([0.0] * len(psycho_feature_names))\n",
    "            discourse_features_list.append([0.0] * len(discourse_feature_names))\n",
    "    \n",
    "    return np.array(psycho_features_list), np.array(discourse_features_list)\n",
    "\n",
    "# Extract features with the fixed function\n",
    "print(\"Extracting features with consistent dimensions...\")\n",
    "train_psycho, train_discourse = extract_all_features_fixed(train_df, psycho_extractor, discourse_analyzer)\n",
    "val_psycho, val_discourse = extract_all_features_fixed(val_df, psycho_extractor, discourse_analyzer)  \n",
    "test_psycho, test_discourse = extract_all_features_fixed(test_df, psycho_extractor, discourse_analyzer)\n",
    "\n",
    "print(f\"Feature extraction complete!\")\n",
    "print(f\"Train psycholinguistic features shape: {train_psycho.shape}\")\n",
    "print(f\"Train discourse features shape: {train_discourse.shape}\")\n",
    "\n",
    "# Save the extracted features\n",
    "np.save('train_psycho_features.npy', train_psycho)\n",
    "np.save('train_discourse_features.npy', train_discourse)\n",
    "np.save('val_psycho_features.npy', val_psycho)\n",
    "np.save('val_discourse_features.npy', val_discourse)\n",
    "np.save('test_psycho_features.npy', test_psycho)\n",
    "np.save('test_discourse_features.npy', test_discourse)\n",
    "\n",
    "print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842880c",
   "metadata": {},
   "source": [
    "Extracting features with consistent dimensions...\n",
    "Expected psycholinguistic features: 17\n",
    "Expected discourse features: 5\n",
    "Processed 0/42380 samples\n",
    "Processed 1000/42380 samples\n",
    "Processed 2000/42380 samples\n",
    "Processed 3000/42380 samples\n",
    "Processed 4000/42380 samples\n",
    "Processed 5000/42380 samples\n",
    "Processed 6000/42380 samples\n",
    "Processed 7000/42380 samples\n",
    "Processed 8000/42380 samples\n",
    "Processed 9000/42380 samples\n",
    "Processed 10000/42380 samples\n",
    "Processed 11000/42380 samples\n",
    "Processed 12000/42380 samples\n",
    "Processed 13000/42380 samples\n",
    "Processed 14000/42380 samples\n",
    "Processed 15000/42380 samples\n",
    "Processed 16000/42380 samples\n",
    "Processed 17000/42380 samples\n",
    "Processed 18000/42380 samples\n",
    "Processed 19000/42380 samples\n",
    "Processed 20000/42380 samples\n",
    "Processed 21000/42380 samples\n",
    "Processed 22000/42380 samples\n",
    "Processed 23000/42380 samples\n",
    "Processed 24000/42380 samples\n",
    "Processed 25000/42380 samples\n",
    "Processed 26000/42380 samples\n",
    "Processed 27000/42380 samples\n",
    "Processed 28000/42380 samples\n",
    "Processed 29000/42380 samples\n",
    "Processed 30000/42380 samples\n",
    "Processed 31000/42380 samples\n",
    "Processed 32000/42380 samples\n",
    "Processed 33000/42380 samples\n",
    "Processed 34000/42380 samples\n",
    "Processed 35000/42380 samples\n",
    "Processed 36000/42380 samples\n",
    "Processed 37000/42380 samples\n",
    "Processed 38000/42380 samples\n",
    "Processed 39000/42380 samples\n",
    "Processed 40000/42380 samples\n",
    "Processed 41000/42380 samples\n",
    "Processed 42000/42380 samples\n",
    "Expected psycholinguistic features: 17\n",
    "Expected discourse features: 5\n",
    "Processed 0/9082 samples\n",
    "Processed 1000/9082 samples\n",
    "Processed 2000/9082 samples\n",
    "Processed 3000/9082 samples\n",
    "Processed 4000/9082 samples\n",
    "Processed 5000/9082 samples\n",
    "Processed 6000/9082 samples\n",
    "Processed 7000/9082 samples\n",
    "Processed 8000/9082 samples\n",
    "Processed 9000/9082 samples\n",
    "Expected psycholinguistic features: 17\n",
    "Expected discourse features: 5\n",
    "Processed 0/9082 samples\n",
    "Processed 1000/9082 samples\n",
    "Processed 2000/9082 samples\n",
    "Processed 3000/9082 samples\n",
    "Processed 4000/9082 samples\n",
    "Processed 5000/9082 samples\n",
    "Processed 6000/9082 samples\n",
    "Processed 7000/9082 samples\n",
    "Processed 8000/9082 samples\n",
    "Processed 9000/9082 samples\n",
    "Feature extraction complete!\n",
    "Train psycholinguistic features shape: (42380, 17)\n",
    "Train discourse features shape: (42380, 5)\n",
    "Features saved successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b05c09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create the interpretable dataset class\n",
    "class InterpretableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, psycho_features, discourse_features, tokenizer, max_len=256):\n",
    "        self.texts = df['combined_text'].tolist()\n",
    "        self.labels = df['Label'].tolist()\n",
    "        self.psycho_features = torch.FloatTensor(psycho_features)\n",
    "        self.discourse_features = torch.FloatTensor(discourse_features)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"psycho_features\": self.psycho_features[idx],\n",
    "            \"discourse_features\": self.discourse_features[idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InterpretableDataset(train_df, train_psycho, train_discourse, tokenizer)\n",
    "val_dataset = InterpretableDataset(val_df, val_psycho, val_discourse, tokenizer)\n",
    "test_dataset = InterpretableDataset(test_df, test_psycho, test_discourse, tokenizer)\n",
    "\n",
    "print(f\"Interpretable datasets created:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef564b9f",
   "metadata": {},
   "source": [
    "Interpretable datasets created:\n",
    "Train: 42380 samples\n",
    "Val: 9082 samples\n",
    "Test: 9082 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c8022",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a custom data collator that properly handles our features\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class InterpretableDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Separate different types of features\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_masks = [f[\"attention_mask\"] for f in features]\n",
    "        psycho_features = [f[\"psycho_features\"] for f in features]\n",
    "        discourse_features = [f[\"discourse_features\"] for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        \n",
    "        # Stack features into batches\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack(input_ids),\n",
    "            \"attention_mask\": torch.stack(attention_masks),\n",
    "            \"psycho_features\": torch.stack(psycho_features),\n",
    "            \"discourse_features\": torch.stack(discourse_features),\n",
    "            \"labels\": torch.stack(labels)\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create a standard model that works with Trainer's expectations\n",
    "class StandardIntegratedModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=4, psycho_features=17, discourse_features=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Feature dimensions\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        total_features = bert_hidden_size + psycho_features + discourse_features\n",
    "        \n",
    "        # Classification layers\n",
    "        self.feature_fusion = nn.Linear(total_features, bert_hidden_size)\n",
    "        self.classifier = nn.Linear(bert_hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, psycho_features, discourse_features, labels=None):\n",
    "        # Get BERT embeddings\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled = bert_outputs.pooler_output\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            bert_pooled, \n",
    "            psycho_features, \n",
    "            discourse_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        fused_features = self.dropout(self.feature_fusion(combined_features))\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Create new model\n",
    "model = StandardIntegratedModel(\n",
    "    model_name=\"sagorsarker/bangla-bert-base\",\n",
    "    num_labels=4,\n",
    "    psycho_features=17,\n",
    "    discourse_features=5\n",
    ").to(device)\n",
    "\n",
    "print(f\"Standard integrated model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = InterpretableDataCollator(tokenizer)\n",
    "\n",
    "# Use standard Trainer with data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_interpretable_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training with standard Trainer and custom data collator...\")\n",
    "train_result = trainer.train()\n",
    "print(f\"Training completed! Final loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAACpCAYAAADdh7LmAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAADFOSURBVHhe7Z1/aBxZtt/PLpuQl7hfNlYHdsdkxlpwrBeDhYUVhsW7GkcQWDPPNBaLIabXf0RIDPsgCAfvXxFC+WuHGBF4yyDh/DEjHDCLTOM3eCCgjDVrluHJaJDBeXIE2/IEzy6k5WxeO3khWXZzz/1RdW/VrapbrW79aH8/0FjV1fXjnnvuOeeee6r8tePHj/+BAAAAAABAIV/X/wIAAAAAgAIQOAEAAAAABILACQAAAAAgEAROAAAAAACBIHACAAAAAAgET9UBAEAPqC+t0cz5ivirTeu3xujRmNlOovZP3+G/67S4NkOj5mc7DRq5Mq83kszSykaNBqPfJI61aa/Twtg0LetNg7zH6iottMYD7s1uE9OkxsgE+e9O3wvF13WPtXGvkYW512x5CK4t0tqNUYquUkp+PvL6Qx+vtxxyzxkze2+Daif1hiGjr3qDat/QswUamypxxbkV2rgct7z9OO94ltM4tWQfJ2Rmt9Xuu0D5HRSlAqcT3/kndPbtf0aVP/4H+hsAAHi9aP/1/6Ann/9nevGr/6K/8RPi6JO/kY6UjNPQTnvX70SiQCTA8Vc/HaGJOf1VhO3Q9FcWqfuXzpKiYEldv+UPnoxjLQgCQmQU7lC5rTV6MWbuJz8oCJGfrz/yg4x8mSbh84+3SgYtXSWkTWlm760QXTFyVsEQ3ffpmJaz6WOhFys0Ef3Ola8iSCcOmODA6V/+6/fp+Df+mv72PzxNX/87JvwGAIDXi9//nzb93//+jF7+7o/pP/y7m/rbNMUOIOFkZYBQpVU7EPF9x+jvWzuDYvaeE0xwAHOx5Q9e8valAgBfAOb7jlHH0g7R4MBWTuAUEmTETrnxRgcONauNIfLzyV4GhHHwmKSs0z+qgVMSXwCkyNIRjUeefRM4/dm/+ff09795nP7e6X9OX/v6N/S3AADwevKH3/+O/tez/0T/87cv6c//7b/S37oUOYDUfq+T9wUXsTPaPJvlsJh8p5XntNP37g9yfOcw392mSZo5nR04lXWQvt9Lhz2Qk9XyyjRbfs75gvvDEBIIuuT1gTofB40L1LoYLxf6lsXkfVtLfk1P9kfKz1oqVb+JA6fV6kx0jvyltzSZ7cgK/A0eGZfVi4OgsDj8B//iz6jyR3+Ljv3JJQRNAAAgYFvINpFtI9vI8tTpwukKNZ9YzmG7Re1K1VMzU6HqKf2noL40KZfvvDN4m2sXaKjSpE3f74RDGz/ZpFWvc/TcGzWp1Xbvw1CpWncsHCEHHrcLna7vGuVpttr6Lz+zZ8W97b5wgqo8+dnnq79RTR2by9wwDba36FFg0BTK4OVJoqURGhnhT4Na52dobamu93Lgo4M/uV98bq1T9fIGrVjtU0ETL6vGv2npfUxFnHP4id53vym2J2nxmt5ZRI4uzU6MEj1u+IMmvveLg9T8NCPoPcQUBk5/8o9P0R+9+U/1FgAAAAPbRraRpZmrycLphu287zyirfYg1e7N6i94Jp8oPhZOapIdYMBsPM9p1ceGqLKzmZEF8NybcG2PnrWFE1+h6O44SHIKm2dp5XKV1pcCHKH3GuVZnhqjkYxsEwcLNeHQHVkVyC/vfDG+ALLzIICDlo2NjegTB0WK9uPbVgZrnhqP21Q5fUFcUcByFMGx054703Rb/GbwrOmpWaqdJ1q/ZWV9xG+mbdnvWIHkXIPWRZA8NObehwtnw/Q9TxHd9maUZmn4ZJu21lyJcHZKtVUFhIUTgENIYeD09f/9G/rGN/+R3gIAAGBg28g2siycCWk/e5Rwsss0Pdag5sla5ETHW+zE2tTa5v3CWd0Yoi3bAWbid1qK/GyP/95UUNHYEYGdcZgXRQAiHHS71RR7OfPBy0phy1RZ1+gW7JzThetl5JeH6Q+LvOxeAbwsFmWLxCe53NX6KqElX8W5IpkVa7eIe8BG/mbghA6uhkXw3aIXOf2i+rAM8zRh7nmJaFLoQzLgy8rAzV8xbb1NNCX0aG1R3ecRAu9xAgCAfSUvqLEckviMidl8taKcXn1pXDjACo3e0IGL+MiMjwy0rEwQk7dsxE4+M9uTd2+20xOfsWm+OeXYZeaDl5Xie5P1NJVRmtlYSyz75F9jb9hLV26AVEp+Aif4KCA3g9drQpYTPcFV17gzTWO31onO1xwZcnCcn4HjicKC0MRRqh2xrFNh4PT7v/st+t1v/5veAgAAYGDbyDayFCVqYaSz1w5ZLiNZQRV/GjtiBz9OnwgS8jI6cgkvK9tTpk5H17bILMvcROreFh63hcNep4WRRBaqzDVKMntPFTr7CovLyE/iqznz3nt36rVCsWuvZHB3cjgV9Dn1WbIdQ3QhtGapG9i60YcUBk5/9V+36W++/Eu9BQAAwMC2kW1kGTioCcoSzK3QDNemrJR1yJzRSS/xKPKzPcH3Js6zciOv8Deb8GsUw3VM0VJPTpFyKM75UjVnGXVMcpnOs3wnkOezatY6wakrkzValk7IeiS3Li71mzvTtLpTodEpa0lM/Gax46BG9L3TJiGXKVcXsjJw/P4n50gu1O9Crdt+Uxg4ffIf/5zaf/P/6NVfPZCP4AIAwOsO20K2iWwb2UaGU6cTA5RdXxQVzoqPfL9N+KPtEddOUJUyZvucMdlZzThn/r3JR8fNvel3K5V5ZF1RcI09Y9VgWR/7CbNw9FLSgKk5U9msVDHzqSpVMjJog9VKYf1Qsjg8uWzYvL9Jw2afCFZb922dSN4j/4bruFy94SXWxi4vm8a/oXLxvkWTWvb1tFxiXeAMnD/gb7aqTv/kva7iMIMXYAIAQAm69wLM/Ybrf3JeRgi6DMubnxzrIPiV8JNr2W/kPrR43s1UhsM3btLgv1wBAIASlPovV+QLB8P+L7aec63gZYSgy4jAZ+0ENTrOqBzNwCnzZZhFSP3sw/+rDgAAAAD7wRHNOL0GIHACAAAAAAgE73ECAAAAAAgEgRMAAAAAQCAInAAAAAAAAkHgBAAAAAAQCAInAAAAAIBAEDgBAAAAAASCwAkAAAAAIBAETgAAAAAAgSBwAgAAAAAIBIETAAAAAEAgCJwAAAAAAAJB4AQAAAAAEAgCJwAAAACAQBA4AQAAAAAEgsAJAAAAACCQrx0/fvwP+u8OqNPi2gyNVvSmob1OC2PTtKw3e8csrWzUaJDatH5rjKbv6K/3TEa7IprUGJmgeb0VSn1pjWbOi5PuNGjkStjRs/c2qHZSiPTxAo1N9UqivZJjtzH36ZHH3AptXOY9RX2TbGtA282596jX+9KXXbrX/SXu107HFjgERGPQpXl/hCbm9IbAjIOu6WhXr+va/vRYtXU1fQ3weoCMEzhCzNPmjvqrUnUN5exZvb2zeQicLhvfDdrYWKPFa/orkM3ccOSISPw1DEfUl/DEcWNDBy/7yF6uWzl9QYxmC0dXwetK1wInjrxHRvTnyMx0s1im6THTnoaYAzOckTDfdTYjXp4aU8cHZpuY+Svqmr3LNh0t5p+o3qCTw2LuZ6jTiQH1V/NJ2Z6ZpwnZp93MtA1S1ZOtRF/6EEHmRXZFbWrutOU3g2fjngVHEM7oyDGlPm5GJu7nrtPV67apzT+vDNEFa/KjJmh6H3ht6WnGyUT6G/dm47/FZ23JieFVqlXvU58Vyylqkr8R53ThFKret7bozhJ6AKd9+VprS4vquvKaJtNgfax7seUhubZIa/J3or3R3+4x8XXUN/a2+Zs/K46RsGTB5zay24Nc7P7zncu+F/5EfZzstz3cg2RuUweyVmbi2gUakoFKkzbFd6l7zc38GFnZv0nIT38bk9PPsh9NKr9CozfEPt3fyb5k8uVq31vRPYVin0d9HN2x9VB+4mtl9vFeiPquRZsrW8IlCZygWJO8L1tO3n1xH0Xti35n2mTJd0npqWxTyh4lx5fAY4+ivrTvrQtjr19QE8cxarT0F/tEp9dt7crIiYbGTM/N0jBnrXa2aEt9YZEeVykflaHDsV2wfQmTtjNdGXNgz3QtcBq8nNO5J2uqtkdTOT8TGSJpbFLr04NUswy2/zc27KDidWeqjNLkPilY5fxofF3xVyrTIO5lJhXkJRHtvTEqWqERx9SShjoBy9BOPQ9eNIONB7AlCz53ruyK4YFt95+E26UHOPePNw3OhmKP104TL9eZzER9bEjJTi/TDaY7QehHaLARIr9O+9mlSK4xCf0Wf413ot/ScNvnUfDYVeNRtN3WQ4vMPt4jTt/deURbMnJKLNdx8JFxX7n7ghHyPR9Lpf5GVf8VM3g5Dqyz7NHy1KoK6q0shVlCbn561LPwJWAdjpz9XoL8knT5uq0nKpCPluv0Ml3zySPecrl2glJaI/xe5AsD9NT1JWyH0nW2bPcRPB08+1PjZKVQG47Tq9OF00ozuAhPpVgXaF0bT+UcZqmmnUv8G/FJLHeZpUJz/mQNTM8wbZPLk2bZR3/u62WlgRMJR5jELAOathNV3ygYHFxgbl+jUlWDLlqD5yJbdR9GJp2hZ1mCeDlWL18KQ+UGePE1k8tR0bFdWMaNluukXGMdMst0ZknMuVdh1k5kZp0sPPJbeKw7JSKnn+9M01hyede7NFtGrvFv9qLfUZBijUfTtjjwVsRjLbksnd3H5Un23TI9eqbvJ1quM0t5AqPz/JF6lLevHKa93KZoSV1+zJisUPUU/5tnj0xQb7IUpo9VJhQcMbZ1IK8DYRUEZ/SlHPdGZ+JxpcZpoJ7avsRjh4ydqZyvCc0CB0lPapxSBnX3RaQgzZZSKIWZubdpa838Yple7Oo/mSiSb9JqpqFuU2tb/eWev/e0nz2ylD+Rrg3OtrTohayvidte5BjbLe2st1tyVmQCg2i2bBVJR4FGJ1jyjw1Gk2wxRzNt0Z813XaZwRDGZNUEyiYjWTIr48Us17FBm3OX6Rh3SSmdYcnDJ7/lr5I5/k772SJArjEe/S4MxtOYTJyts27b5qlhDL6Y2cq26exXZh/vhWiZLtaPKAMXLdfFNmJ9xQ3h8veVwbY/Qgc4o6TbuJGc9RfYo/mVdTkeZZbCOL9D8cDCPuLUGu3jE5Jdv64J5DkQXtTLdBl9mViGczPJYXpqj0ufHYptPTho9ifjFBl5K8MkHb9xFPY6clzoK7nzQoQVjJ2+FxH80uGLuetL49JQmiAynanYR6w6keiJs07wyj+5VBVnYKKM4mWVKo+yPyYrc7K2d4crrhfN7C+6y3RswMalgdMzu1vKkZUmR35d6ecguXYXE3TZTwoll6WibIuRW8Use2f3cadEGTAvRi4+GyH6ZIkDurx9MSZ7m389g84oRU7YZA81RfbITBZEUD+pi973FtSBg2R5TS/X6WW0rIdPZid4GU5nmK0xogjTU5toQmPX+52qBugv2A96UuOUWl8WxletPZvZm5nhxan5aIYb/cbM6Kyaluga4jeuvT9UpGbP+4gZ6GzYTWYgvDZFFzPr41S2wSd/ncURzqUhnIc9Q4+u1W5R056FRVmZOHuyF0wWrVJRMk4ZNBGgyeuWrH8pI7/iftby9GbZiuW6J6Ixpz5cFxG1zdpn7l3V4FiZNEtura+Ws/tY/1ke3zJ9IjCTy3U+GyHu4TR/k78vyt7qfTPie/XrACIZJTOWxfZI6WZF6Kb4p71Fjw7te9FAIVHdHVO05BrbT9dm5OlpBtZDMMYOGRvaftwQWggOkv3JOInZvx2BN++PRY9+yxmuyUYY5GwvTrVy1iI5q4+Wqg4R8XKGgNuQbNd+wGvt9nXZCe8x8+WTv8zoZNaSNKmRsc/u+z0RGRbGMmjW8qBovJgBJjIGRXjkl+zH4n6Ol7zyKC/XPcJtS2ZQpIyyX+LHQY1/X3YfBzFXS0yiYuJXTozLgmyvjdDL/3n75q/YbRX3u5R+FiqN23fN+3HdoaHQHs01omNeq6LwviQOevKWXM0SrUSM4aR+5OmpH87wpnWPs9x4ncnBs8c3h+fDs1Q5o2VnUOLdRaCbcBZBz5rRDwD0Hn6CSmYHuLB3H2t8AAD7wv5knMA+Uk+8+8MsNaDWAoCeYt7ZhCUVAPoaBE6vA3Lps0tLZACAQniJE0sqAPQnPV2qAwAAAADoJ5BxAgAAAAAIBIETAAAAAEAgCJwAAAAAAAJB4AQAAAAAEAgCJwAAAACAQBA4AQAAAAAEgsAJAAAAACCQwvc4Xb16Vf8FAAAAgMPO3bt39V+gF+AFmAAAAAAAgWCpDgAAAAAgEAROAAAAAACBIHACAAAAAAgEgRMAAAAAQCAInAAAAAAAAkHgBAAAAAAQCAInAAAAAIBAEDgBAAAAAASCwAkAAAAAIBAETgAAAAAAgSBwAgAAAAAIpM/+r7pZWtkYps2RCZrX3xxZ5lZo4/Kg3mhSI6RN8pgqrd8ao+k7+jsJy6VG0dnuj9DEnN4wXFuktSmi22PTtKy/YmbvbVDtpN5or9OCs79Oi2szNFpRW+3HCzQ25RxdfN3XgPrSGs2cN0JKytCPlPuA+1unL6jt9jP3341R0lch2mnQyBVLYxx9SveFc48SV+fc/YH6WJpwfQm5H0detjwKZJF/H+6+dH8m9gvS46IzwvXIvYfk9V3ZJfRIECq3GEv+jh6mz92RXeuYcH2K0PffyutzgSPTQn1SsNwn6bZXFzJl3kkbQM/pm4wTK+VGQrmPLDx4ZQA0QiMjI7TwuEq1tUURpuQhgpiLntbzuYRcSAw4Phd/kgOPB+2G7XQ1LFPpvPVxjd1Rmrk3q/fycSJo2hWDXO5vUOv8DK1E5+agqkZVYWDk/lvrVL28RovX9O7XBWFUZ863hIPwy9CL6LPxKEAyzNIwGVmzThCNTsU6UR+r0pbWF+6L5sma2xdnW1E/+vpisFqRzsCcf8R2aIk2hOljWUroS+H98LmEI7LkFTuiIlkU3MfcsDWWFmidRmlyyZLEtRNUlQGB+c1IV4KmcD1K3H9qXM5Srboa3dvI/RaN3lgR3+q97MAtuTXE1ppp39xEfJzZv8NBRMMKmoZiPby1RUPWuTuza51SQp8sZifSdjC/T4v0ieHgZ8MKVm34PrN0VQVNsb41iALaAHpPXwROcgZ1eosWhNK29XdHmfrYEFV2VqOZ2vLUKjUrQ3Qhb8DM1UQQ0xTD24UNAQnjkTVLiQzl/eSR2pk+exTNauefiN8MnNCGTjjyk2JGuWIG+Tw1Hrdp8Kw2k9cu0FClSavGwNyZptWdCg2N9cZMHlZmzw7GjkUwvyJ09ORw7Ew8cJ+1dpL9MU8TkUEVOrG2Re1KNZooLE9NWDP7edoUDq36hpH1Mk1fsbITGX3R+srv4OtvVMUseDNqQ/LaXaGEvhTeD48FEdIsWPKKKZBF0X2I4CEeS8v06FmbKtWEJNqt1DjcK+F6NEjVSpu21kwLk7rg6hHNNWi9PUjDsk3JMa2uQ+drfn2VAX4sq6TdUrIbpHEdeHVk1zqlE/vD7RkQNtTnRDL7tGhsxcEPB5kp8nRVBOmD7XVqRPrGNlZMmCbyrAfYD/oicFqeGqORgOWPo0GdLpyuUPOJPZDY+OUNejE4eSa3sqm3DWwILePhYf6KPcNx4UCpYhlNabxNICUH9RY9stLw0oFpY66MZOzcGHm+0xd04PU6oBxR7MQEdx7RVuSoPMytyCxf44nezkAFxLEjdUg4tDR1OjFgB0pqOwvVr+PRTNfXt3uljL4U3Q/rafPTUHvgyqKc3s5S7Tw5gQadqqYzFnumjB4pWxE7V8+xDirQam3rzSR3XlCLqnTCE9zk6qCm2TKBZSd2rXPK2586LU6JCcunDdHeBKX6NDm2RKA6kr28Vk5Xhe5/Je4umryCgwLF4X3A7D1OSd92awkYTjGLmRLJZcwN9SmTGufU/H2imj52vNWdWg2QhQ6AlzIMqQiqTD8OP0kuAamUv9zPtWp5tSM8yxWz8c2EMR+8rI/fSCwHiFn0mFx2UftneKknI9jeF3Lvhx2XCASIl6gz2mOTIYs85NK2PC/XUyZqeJjKKM2Yaxcty/YAngzxEpu6x3FqpWoeY+pL49YEKBl06f36bwdPcJ4MaFmfa97lqcNHfWlSlh1k1g+F9mkpfSrQ1blNaorr1qJziTHuK8cA+w4Cp6OOzlDczgpoxMAbp9vR+nlQjY1G1o2d3YyOvU2T5QIvUIrMANhg1ZhsnhVG1umLZZoeU/tGlogmhRGO61osOPiSdSZ2YGUdyx9Z92IbcHGMDMb0/ifDwsBbtSv7TeH9COd/0dqfqOOJ8MqiGJmllefepGEh56gGiHHqgBZofUAEMPsaPKkAmgNrdQ+3iaYS96iRJQ5cN2Vl6+ev6HvWjnyStrxLVJzRSWWbOKCVsjZBwDC1Hh+B4gkRBE6yHLImA6F92pE+5enqPE3Imikjz0miZ91eBAadgMDpCJGqQREDXhZbZmUoJO6sMKTGRpFehlieuk3rVFCTUFTfsfsi515fF9JLI6YQPzMATiAdHNmzUQvpwJo0eNENcmWm5CIXsmZnICTCUTSiJRQ1y3WWE+T+nOXGbpLSl7D7cffbdTyKYFkYvHqrHFtmDZA4YnopdLx1gmeJTdfMxHUx6h7IWaJSwZXM1KUyk24QPbZGVKUWvXBkxMtu5F/+c4KMCXrBdZKtfGefVVvXE1L9OEsr8im60GDH36el9ckiV1d5LEfyHKNHojdgQw8eBE6HjmV6sWsXczKqTiHlbHkdn2cs0QyPnypU23KGKesTesR2K1UgLO9HD2rfWryskSowov1Fk1rtClVP6U1GFq36HFHFXQ7gx5vldneeomHDzkutZWoB99OhdU9f1PjJI08WPdHbPReLh+pRCBw0zVD1U+GIA5ZbfbVC8trk1jf6seurwu1aNwjuR67VFP/Ey9Tq9SpyOy9TaPVpJ2NLUayrLr46MXAQIHA6hKgnWSYjh+nUIXCWSTtTWRQfzUb40xCDmd+dYupfuGZhkGqWAZCP2yYNoRdfvcOkmM3q+xAzIX5iJj63qmeIBjXPnMSvo0e1ZU1EXpFqPyJmp582hRGOl4kc+XNqXy4xubN8+eGnHPldPWYGK35rL73JvohqKYQzvGdnl7hWyirk99SjuMzSorPctEI18Xt1bvXkmJO9cvZ3iQJ9kTN6qWvF98NFwLbMnbqTIlkU3oe95CfkPuWOp/rSYmo/WU+mdkaoHvHfnroY+x5SGakcRNsnk8XvAnuClAcvPQ9aT9Hl2rVuk9uPHDzqyaWTIePPAq23RajKrwDQgWVunxbpUwG5uppA2d/AvgM9pb9egCmUeO1GlVZ7+lK1fYKNIWcdGPtld7KN/K4UX0qYH31NF4Oy0/G/XM2Cr8ep5sSsyTlWhGXuC+vYAMUvwEy/nI3vx7xby/MyvNcEVUuihZR6oSAlZKpJ9ofsd/sdM25fuP0kpP3YKuRPHauJ9MruJyb9YkKnDT3ry2x9ke3j12ZEzqzgfuzxY7enUBZM9n241xUkxlNyf7defsmE61GiP1O/jXs6Qv8mpJ+5L7iGKl1InXNdg319R+a9IKsfld0aeubrG7WPM3Kmfbl9GqRPCpab9wGbLF1N2NfeywuE0mdvDgcAAAAA6B1YqgMAAAAACASBEwAAAABAIAicAAAAAAACQeAEAAAAABAIAicAAAAAgEAQOAEAAAAABILACQAAAAAgEAROAAAAAACBIHACAAAAAAgEgRMAAAAAQCAInAAAAAAAAin8v+q++c1v6r8AAAAAcNj57W9/q/8CvQAZJwAAAACAQBA4AQAAAAAEgsAJAAAAACAQBE4AAAAAAIEgcAIAAAAACASBEwAAAABAIAicAAAAAAACQeAEAAAAABAIAicAAAAAgEAQOAEAAAAABILACQAAAAAgkP74v+p+8iE9/MFbeoPo+Sfv0PWf6o2jitOm5/Tgnev0vt5y+OECffzjc3RMb9KXD+idH9m/vEkfPrxE5kyvvviA3p25K/++uvAxvXcuOjLm1Rf0wbszxL+6+dFDuvSm+jp97qu08PF7ZE5hn1vhXrsv+qUDHDlbss1Dyv24+1u3v17RFz97l2Z+rjc13v5KjI8Yj16xPl0nWnbu0e3H0DaUJ1xfXFn4x0em7qbkEcsyf0x8Tm9b+m5j675z3S7KKlyPsse8ix6/L5PjWsHtOPM02QdFfZS337UX3ZSNn3B9itD2dDfx20xdSiB/54zbsDYXn5/bcoaeZvmBBPi/6npLH2SchGKeeUkfvPMOvcOfn31BAz/4mBZ+qHcfRXjw/mBAGHPVpg++GKBLHy+Ilqa5+t3jtK1/9847D+j5m5fow5/onXLQXqIBYTjN/t1z70X77868q783nw/oi1fCwPxCDWw5mEkMYr3/gdj6eCG+i5sfaaMr97vnTl27H/qlE4STfu/crnDsWoYvz9F7H93UOzMQ/f99Y0QjbtKlgc+0rMXnk1069+MPxbcGlrfbX5Hx/en1+Dv9efAl2/AHjhHm/n5oB+GGn5whEo5EHSt0hM5R3dKD7lBCXxIyTY+PHFkIrn5rQDkns++dOADNHxN3aeZde5/4iPt8JQK3z6zJiHScen9Qf4cQrEf5Y97hJ5e8QaC0Pw8tRx6hApFYFx4QOX1U0Ifiesd/YY7tomy8lNAni5t/mtb/IjsY4Ru3hW3O11WGdeqhFQCCg6cPAidhzH5kRfA/n6HPvjxGp77bbcO+f1z97ik69uVnljH/jJ4fO0Vvewb93ZnrVtbhfXoqHOLAt0zb36Ljx17R9i+NdJL7E7AhFW7xgZxp3aQzb4qZ+F/Eg/j9v/iC6Nwl7ayT+9+nB8LDvHVGG4Ufvk2njsUOpR/6pRNunnnLCVBYhq/ePGMFPGnYeO9++VxvGd6n67ZB/ekD4dDfojPGIeq++yA1U/UgDbzVN4LIOXySvK5ABF7x7Psufb79io4NdNmMl9AXFfg8jWR695fb9OrY8dixBMji1a6nnT6cMZGG+4qs/n1r4Bi92v48skfvPxXXOf5t4R73RrgehY554bC/N0DPvxRRoQ0HTT8+JSZjKmB0EAH0W69sWfCYFybhTwPHvKNHeW3oAp3YHx4Xx5/Tc6fdRXYwxjtui9pcoKsyy3hqmz6QATo4LPRhjdNV+vZxot3fGMNx1LhKb586Rs+f2gOJjV9A0JFyiOq4yLBpIxAbVRs2pG9F2SYvP/817dIAfZsDOGlEt+lza6lIOjBtFFTwFzs3hp3IsVNv79mJHB088v7557RtBzxJfvKhzFg8eKq3M1EO8uWv1BY71ty+s0g6e+b9H6Vnun5u0iU+3HIk3aCMvig9+36UPUgeWyQLDm7CKBgTngBU3rPlVGXAYwVSnVFGj8LG/NWFOp17+Rk92NVfGESA8a6VgSvi7m/ECXRgWHrMf+c4HXv1kgJD2FKUtz+ir6+LwOcXD+il/iYT2w4aQsdtos1FuiozoD1dzgSd0H+BE0fwYqbxNGOG2H+oVO/Dh+LDtSmJNXB2iJxalvsffp9eeupiJKmZddIAs7H9PtLFPeUmfchLtB8WG0rZF1HgypMFEUSRWmJRfZ2xLOFx9iHIZTx5Xq6zCHesPYGd+8+26dSP1T29x0uYUdAXJotj597T+x76l7GYktkmCS+LfkJ0SZ/7+7tZ9UW9o3DMCx2o87JfUKBs8dOn9PzYOboUyUsFlp2hjt17UNkdVCD5wFMDFWIHQ8dtss0lxi04VPRX4CSi/oeyNiisgK4/sOouPiSqi8Hn1hmpAk+1fr5MdP2hd33eN/N5/0cf0BfHjQF+SHXa7snsEChufsQ1GcuFQYkqEhaOz5mJCuP+PQ6cdV+naqAUPBNPOfsAZEZKnvspnRG64K3x2C94nMtJgr6np2eEftptzZdF3BbxkbUv/uApPxvAmZx0ACrrUc48jc6/LEbNw4z6xN5QNObFfs6sfNKJjXyfrmt5KZtQJ9ruxCLwPar6yP0OKr0UBJJFdjBs3Ga1OWzcgsNF3wROckb8PS4SP+DZcA8pXH7kmfgnz+mt72lDnZoxiyDrwy+IUulq5QTSWTq3GPbdXxIN0C79Ok++Ran3l7/OcESvE/ESm8EUFS/nOhLlFGWGxfN0jePkkzVQEl4GFr7Ou1QbinKevhqPnpDSFyGD5PLZT6/Tgy/dthbLQiPGzLJdmxeRNSY0vFSdWAriY5LLmHdnlsUI9Ncn7p20HhWNefNARzqzEohcyjM24V36XFiEwjFt7xdByscP31MF02UzXt0gda836UP5FF1eIJltB4PGbUGbg3UVHBr6InDioIlT4v2xFnyXfv0yWcyp6hRSRrJbeJ1AGqdu4Fcv3YJcgdyvDZNd+2CQ9R6hRbl9wXN6+eoYHf+O3mRk0Woy+FR1bXTsHL2nZ7XycXm5bVL3asbqN75KZwrha4vZsl2Xdljonr4EyiJB6joFY0Jmo5w6xF4SqkdFsB0R/7wZZ0/k6w3kdidZDrces7APOYD48XH6TAQfHQdugQTrE/ez+OetKIsmAkshErmd8cRfbAcDxm1umzvTVXDwHP3ASShmJzUbhxn11EY9Wut26lnk7MVyph/ZywC81m6toftqEq6LKXGiriDIQYnr1u3ZtJh5fiZm+Zci48KzbauonWdOYu4bPbYu+ymrML1fETPVXzwXRjh2SlwXEwWfvOQkHZY7o5UffrqN3/liMqgF9TZc+GpfR/4+UetnB7ZluPmR7VSVDiULb/dMgb7IjLLUNfVUX5RVZbgw18oO5ctC3P9C3Bqj10m9zB8TWRMZXz1MXbRqr8FqqB4Jcsf8+3Td1jHx+YAfnZOvZii/dKfaZulkUR/6asJ6Re69CJl8rJcvuSbNkYl+/QS/csGXEdP6ouxg8bgtanPIuAWHj6P/AkyhyM4LIA09f7laj5H1WjqfY7dFtpcfF9YDkx8jt94dUvQSyvTL1diI1Ik+TC9xqloaI1nfCxf5WDVDY/Jfhuc7/vXAkaMtf9nH5H15o9zHS8+m3219sEmdz/wm/VJI1pX0Cw0TJK8rcPVAkNKhbpGtL1LP+XUJ+rqFupkpC1dns/XaPyYkPAZTLwmNccdkzstrSxKuR0VjPkae0ymuNyg5cYYz1peE7Lw2tqAPU++G8tmNbpF1L6odp7Z9hfvpdhfqmk1i/AS1uWDcSqTd58xVmC7hBZi9pT/eHA4AAAAACQKn3tJ/ryMAAAAAAOgRCJwAAAAAAAJB4AQAAAAAEAgCJwAAAACAQBA4AQAAAAAEgsAJAAAAACAQBE4AAAAAAIEgcAIAAAAACASBEwAAAABAIAicAAAAAAACKfwvVwAAAAAAgAIZJwAAAACAQBA4AQAAAAAEgsAJAAAAACAQBE4AAAAAAIEgcAIAAAAACASBEwAAAABAIAicAAAAAAACQeAEAAAAABAIAicAAAAAgEAQOAEAAAAABILACQAAAAAgkL74v+rqS2s0c76it4ia90doYk5vHFXmVmjj8qDeaFJjZILm9ZbDtUVauzFKUet3GjRyxfqlcx6PbBLHu/tnaWWjRvHRRO3HCzQ2tay36rS4NkOj+mB3H+Me3xf90gGOfrbXaWFsmmwp+Zi9t0G1Afe38ruTeoPatH5rjKbv8N/pfjJImZOrAzGxXrljyD53Aq0vrZ70Zbi+uPfrHx+OvJxxUaS3Gm7rFNFtT39l9mnReNwD4XrkyjGzfUYOu+l7zJRdwp5k6kqO7CQ91SNDqD65+iCx21zYp3nXSZw7p99Y5sNPkvcYPibA/tEHGac6Xahu0cLICI3w536TBi+vCHU7wvBAvVwVBkm1aeFxlWpri6KlaepjVdrSvxsZaVDzZI1W7EF7thXL5tY6VS+v0eI1vZsHpTZe3v3XTlBVOiW9X3xsAzx7Txtdua9BrfMz7rXXalQVRtt/7dcE4WhmzrciGTZ2R2nmXoF2iv4fjwIkwywNk5E16wTR6JTRiXma0N9HHzEO2Eg3uD/mJtx94tPYYYfa0MHGLNWqq/H++y0aveEfQ7MTlgPpKiX0JSHT9PjgcwnHb8nLdnT5eqtgJ7ZhO0sL3jdz2rI5liPMH497IFiPEnLMaJ9kruYGC5J82dXfqKrAwewbSQdNebIz9E6PDGXszyBVKxwAmjaJj93m3D5VgQ0ZGyr2k30dIePqp2ZfRr+xvd+wJ0WGgnODA6MPAqdlmr5iRfBzm8LVV+nEEVau+tgQVXZWI4O0PLVKzcoQXfC0aXlqwjJc87QpHGL1DeNCErK5M02rOxUaGtP7dWC0aYzAnUe01a5Q9ZTeZtot8QsfwpGfFMZmxRiYeWo8btPgWW0Url2goUqTVk2glbz2a8Ls2UErQBFSWlmn9slhb1BiYKfS2klKXQRHljFfXtuidqUazURdhNO4OEjNT7Nn++Mnrb5JnJvmGrTeHqThpLPl4waa1Gzr7W5SQl+U896MZJqSBQcEJGb2dpsiCvRWILMtHDhw8JlEy66RkTXIH4+dE65HKgjYWov71n8PrCNi9O8kOjNXdop2y28RmFzZGXqpR4bS9qdFL5JZM01un84N06CZoEhYn8SkZkL3jJi02BmiVL9x0HRjSARmC2LM6e8MRecGB0bf1TjVl8Zp0Ao6jh51unC6Qs0ntuHiwRoQdKQcYpI6nRgQJuIrY0w4UBqk8SV9Xm1sokDqVDV7VigH9RY9suQsHZg2Cir4i50bM/+kSZXTF8RdvC4oJx07MYGWeSooMcytyCW6xhO9nQEHV2Q5Ugft/GKD65J7rEQ539a23pQIRzslArpPG8LFdJ8y+qL0bDyaeSeP5SAjM2gs0Ftm/oqY3WcEDnytfNlZFI7HUMrokbIVsXP1HCuoL03S6O4qNRKdmSs7wWA1P0+UJztFb/XIUMr+yAlkIAF9uvyVaNnACb+dY5tqT0ZFQDfmydplkXtusG/0R+DEUfvGBm2IzyTdLhi4/QanpFXbN7imIKsWipGpeSswEuZxemyBtk7PqONvVGk1eXxllGa0bDeKlpjAHpmlFV6iXcpy+iuqH8SHayEy61ZKZZvSyMlHIrhQjrZxOOor2Nnc2qKhG0oWM7zMGI15nhyIoI9im7Cx0b3lDQ4cWl8N0kp07o3EMliJ8dgjOHhpUE3f3zi1kjVIQgcmedkvZSfDZFc5r+2F+JRdhjxUeuQwSDXT5lRZRE6f8gqHsJG1qD1q/PlR+9rPHmUGpg6lzg32k/4InGTUrtaBb4vQaWPjiNc4lYKDH70GvkQ0mWXM2OnKuinbkPMa+iTRkj5+ZJOG7eOd2pgFWh8QxhjBU8+Yvcc1GbezZ59Wf2ye9Rl4AWcNc7JNRRkTVYAsnKq9FJXpaA8I1mXpwLRuPhlOjPkKjV609ufUbHXC4OVh2jTnTtXOBI7HnqGcPAfW8h5GbhNNbdCaySrzfs743M8K6PJlJzNKZp9se4n2HTY9Mlj+gz+yDskZW3l9Ok8TWg4qmBT29JlvmZL7RdXW+Sc8PkLPDfabvluqW54ao8aOtfzUR0RLbFmwAeDi+IuuQ5XFmhe5SNydeaaXNcVA5eOteo8YYTyWiutzsmuiNLsvYof82pJcBlMBCy/R3Q40qvNXRCBL9mxUIQOjzBktLwNTatlGoRyuzN44WRLzAMH+Z04kKX1Rs24noyYCSh7z9rKVuz+jZsumSG8t7FojHnOZtTMZ47F7pPUovUyrxi3pJSpTGJ+X8QmWnWjf7UR9WDYHrEeGAPszf6WRWVPq7VP+Lgq8xugRL/zZ15ErIjOqSLxs0Fh0bnAg9F3gdPRZphe7yWJOVaeQMpIBcNA03lpwnvzZE8bBbLdSxcmyrkAPat9avCxwzSks7T+a1EoW28s6smQhqqprc5ZFLwvJyu0yy0x5gZFAZqPcJTiFmg17DTvXBIl/BqNZr3q0Wm53MfvYPX1R4yeTAr0totnqZUVzFqF6VATbEfHPSbOMJwJlfr2B3OasUoHsMgjqo33SI8Pe9amsbA2JGlUOmmQJRDdeI+CrfwUHwdEPnIRirtjZJS6u9RRDHiX4yQs6PxkXv9o1J3L2YpypcHj37Nks18hYa+jit3n1LMkiW3O8GZj1pUXxjUFca2o0zmaImdCqmOXXIqM3SzVhhKNBzTNVMfedjArP+V6Odr+UR8z2P3VfjyEfwzZFq7JmSTmsaCnAfPipJH7ni8kSit/aSyKyVsSpVxMUONPMACGvmDz1KgP19A+/T6artYQF+iKzplLXlunRs7Y745djPpYFFwE7rySxa/uK9LYAHjP22HTvs2A8dkyoHvHfnrqYaNzOp15bsfBYdKZ8vYDKBOXKjs+1FO2RbZ88nxOo2+yXHhly9Um0Y81avpxbdCYnvGQ+GBWWl+tTOS6tsVT8IEY4yXODg+PoB053XghDFhcrqjqe8KcUDiUyHcy1BapN8p0x3oyRmCGKoRRlKTbUe0vcNXSr6NF8zPo9X8cqsjXHxzOjIevYGRp65p5bLhdx3ZM+lt83Eh/LwYB6h4zcLx+5PeL90gnCYcj3DGk5yke1O3EU2y2r1oEzBfxOn8SyBz+xk3iSyIYLmzMDBDvbZT49yARkE64vvBy/8GzIys4lavfYSd+nWHcvkyOrfL0tIDlmnPsMGY8dEqxHHBzx+37MPZSsqymQHZ2Os1WHe0yXsT/VyNbyx5VtUZ+qIMwc67PVdjG9+YTVhRWfGxwMffHmcAAAAACA/QA1TgAAAAAAgSBwAgAAAAAIBIETAAAAAEAgCJwAAAAAAAJB4AQAAAAAEAgCJwAAAACAQBA4AQAAAAAEgsAJAAAAACAQBE4AAAAAAIEgcAIAAAAACASBEwAAAABAIAicAAAAAAACQeAEAAAAABAIAicAAAAAgEAQOAEAAAAABILACQAAAAAgEAROAAAAAACBIHACAAAAAAgEgRMAAAAAQCAInAAAAAAAAkHgBAAAAAAQBNH/B/H8Y9bOSy/OAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "70d37355",
   "metadata": {},
   "source": [
    "Standard integrated model created with 165,007,108 parameters\n",
    "Starting training with standard Trainer and custom data collator...\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !\n",
    "Training completed! Final loss: 0.4156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c3884",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate current model state\n",
    "print(\"Evaluating current model...\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Current evaluation results: {eval_result}\")\n",
    "\n",
    "# Test on test set\n",
    "test_result = trainer.predict(test_dataset)\n",
    "test_preds = test_result.predictions.argmax(-1)\n",
    "test_labels = test_result.label_ids\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "# Save current model\n",
    "trainer.save_model(\"./interpretable-model-partial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532f05d",
   "metadata": {},
   "source": [
    "Evaluating current model...\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "Current evaluation results: {'eval_loss': 0.4377244710922241, 'eval_accuracy': 0.8601629597005065, 'eval_f1': 0.8465144415369399, 'eval_f1_macro': 0.5392189033207623, 'eval_runtime': 103.123, 'eval_samples_per_second': 88.07, 'eval_steps_per_second': 2.754, 'epoch': 3.0}\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "Test Results:\n",
    "Accuracy: 0.8573\n",
    "F1-Score: 0.8437\n",
    "The OrderedVocab you are attempting to save contains holes for indices [1015, 1016, 1017, 1018, 1053, 1054, 1055, 1056, 1057, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1101, 1112, 1113, 1556, 1557, 1568], your vocabulary could be corrupted !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501162c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install huggingface_hub if not already installed\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Login to Hugging Face (you'll need to create account first at huggingface.co)\n",
    "from huggingface_hub import login\n",
    "login()  # This will prompt for your token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a2e7d",
   "metadata": {},
   "source": [
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
    "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
    "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.5.1)\n",
    "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
    "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
    "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\n",
    "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
    "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
    "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
    "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
    "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
    "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
    "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
    "git config --global credential.helper store\n",
    "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b556af0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will prompt you to enter your Hugging Face token\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ef9c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check both interpretable models\n",
    "models_to_check = [\n",
    "    \"./interpretable-bangla-fake-news/checkpoint-7947\",\n",
    "    \"./interpretable-model-partial\"\n",
    "]\n",
    "\n",
    "for model_path in models_to_check:\n",
    "    print(f\"\\nChecking: {model_path}\")\n",
    "    if os.path.exists(model_path):\n",
    "        files = os.listdir(model_path)\n",
    "        print(f\"Files: {files}\")\n",
    "        \n",
    "        # Check for essential model files\n",
    "        has_config = 'config.json' in files\n",
    "        has_model = any(f in files for f in ['model.safetensors', 'pytorch_model.bin'])\n",
    "        has_tokenizer = any('tokenizer' in f for f in files)\n",
    "        \n",
    "        print(f\"Has config: {has_config}\")\n",
    "        print(f\"Has model weights: {has_model}\")\n",
    "        print(f\"Has tokenizer: {has_tokenizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486decc",
   "metadata": {},
   "source": [
    "Checking: ./interpretable-bangla-fake-news/checkpoint-7947\n",
    "Files: ['trainer_state.json', 'tokenizer.json', 'optimizer.pt', 'rng_state.pth', 'vocab.txt', 'tokenizer_config.json', 'special_tokens_map.json', 'scheduler.pt', 'model.safetensors', 'training_args.bin']\n",
    "Has config: False\n",
    "Has model weights: True\n",
    "Has tokenizer: True\n",
    "\n",
    "Checking: ./interpretable-model-partial\n",
    "Files: ['tokenizer.json', 'vocab.txt', 'tokenizer_config.json', 'special_tokens_map.json', 'model.safetensors', 'training_args.bin']\n",
    "Has config: False\n",
    "Has model weights: True\n",
    "Has tokenizer: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186fa4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create config.json for your interpretable model\n",
    "import json\n",
    "\n",
    "# Generate config based on your model architecture\n",
    "config = {\n",
    "    \"architectures\": [\"StandardIntegratedModel\"],\n",
    "    \"model_type\": \"bert\",\n",
    "    \"num_labels\": 4,\n",
    "    \"id2label\": {\n",
    "        \"0\": \"Class_0\", \n",
    "        \"1\": \"Class_1\",\n",
    "        \"2\": \"Class_2\", \n",
    "        \"3\": \"Class_3\"\n",
    "    },\n",
    "    \"label2id\": {\n",
    "        \"Class_0\": 0,\n",
    "        \"Class_1\": 1, \n",
    "        \"Class_2\": 2,\n",
    "        \"Class_3\": 3\n",
    "    },\n",
    "    \"hidden_size\": 768,\n",
    "    \"vocab_size\": 30522,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"transformers_version\": \"4.56.1\"\n",
    "}\n",
    "\n",
    "# Save config to both model directories\n",
    "model_dirs = [\n",
    "    \"./interpretable-bangla-fake-news/checkpoint-7947\",\n",
    "    \"./interpretable-model-partial\"\n",
    "]\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    config_path = os.path.join(model_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Config saved to: {config_path}\")\n",
    "\n",
    "print(\"Config files created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac66d5",
   "metadata": {},
   "source": [
    "Config saved to: ./interpretable-bangla-fake-news/checkpoint-7947/config.json\n",
    "Config saved to: ./interpretable-model-partial/config.json\n",
    "Config files created successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687946d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare both models after adding config\n",
    "for model_path in models_to_check:\n",
    "    print(f\"\\nChecking updated: {model_path}\")\n",
    "    files = os.listdir(model_path)\n",
    "    \n",
    "    essential_files = ['config.json', 'model.safetensors', 'tokenizer.json', 'vocab.txt']\n",
    "    has_all = all(f in files for f in essential_files)\n",
    "    \n",
    "    print(f\"Has all essential files: {has_all}\")\n",
    "    if has_all:\n",
    "        print(f\"‚úì Ready for upload: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d82275",
   "metadata": {},
   "source": [
    "Checking updated: ./interpretable-bangla-fake-news/checkpoint-7947\n",
    "Has all essential files: True\n",
    "‚úì Ready for upload: ./interpretable-bangla-fake-news/checkpoint-7947\n",
    "\n",
    "Checking updated: ./interpretable-model-partial\n",
    "Has all essential files: True\n",
    "‚úì Ready for upload: ./interpretable-model-partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ec9ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi()\n",
    "\n",
    "# Your repository details\n",
    "username = \"NUHASHROXME\"\n",
    "repo_name = \"bangla-fake-news-interpretable\"\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "print(f\"Creating repository: {repo_id}\")\n",
    "\n",
    "# Create the repository\n",
    "try:\n",
    "    api.create_repo(repo_id=repo_id, exist_ok=True, private=False)\n",
    "    print(\"Repository created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation: {e}\")\n",
    "\n",
    "# Use the more complete model (checkpoint-7947)\n",
    "model_path = \"./interpretable-bangla-fake-news/checkpoint-7947\"\n",
    "\n",
    "# Upload the model\n",
    "try:\n",
    "    upload_folder(\n",
    "        folder_path=model_path,\n",
    "        repo_id=repo_id,\n",
    "        commit_message=\"Upload interpretable Bangla fake news detection model\",\n",
    "        ignore_patterns=[\"*.pt\", \"*.pth\"]  # Skip optimizer states to reduce size\n",
    "    )\n",
    "    print(\"Model uploaded successfully!\")\n",
    "    print(f\"Model available at: https://huggingface.co/{repo_id}\") \n",
    "except Exception as e:\n",
    "    print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6b89d3ec",
   "metadata": {},
   "source": [
    "Creating repository: NUHASHROXME/bangla-fake-news-interpretable\n",
    "Repository created successfully!\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Model uploaded successfully!\n",
    "Model available at: https://huggingface.co/NUHASHROXME/bangla-fake-news-interpretable"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
